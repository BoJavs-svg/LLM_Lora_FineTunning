{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BoJavs-svg/LLM_Lora_FineTunning/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys3tpOFBeUHf"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip install fastapi uvicorn pyngrok nest-asyncio\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "login(userdata.get('hf'))"
      ],
      "metadata": {
        "id": "B7aLHYR32yD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load the base model\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-14B\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")\n",
        "\n",
        "# 2. Reapply LoRA the same way as originally done\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    base_model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# 3. Load adapter weights directly\n",
        "# model.load_adapter(\"BoJavs/TrainedQwen2.5\", adapter_name=\"default\")\n",
        "# model.set_adapter(\"default\")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]\n",
        "})\n",
        "\n",
        "# Set the chat template manually (Jinja2-style template)\n",
        "tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
        "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}\n",
        "<|im_start|>assistant\n",
        "{% endif %}\"\"\"\n",
        "# 4. Continue with training or inference\n",
        "print(sum(p.requires_grad for p in model.parameters()))  # Should be > 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:\n",
        "\n",
        "1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.\n",
        "\n",
        "2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "swe_bench_lite = load_dataset('BoJavs/Clean_SweBench', split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZICZtie3lQ"
      },
      "source": [
        "Let's see the structure of both datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zoaygOAe3I2"
      },
      "outputs": [],
      "source": [
        "swe_bench_lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OMhyEXkfM5e"
      },
      "source": [
        "Next we take the non reasoning dataset and convert it to conversational format as well.\n",
        "\n",
        "> Agregar bloque entrecomillado\n",
        "\n",
        "\n",
        "\n",
        "We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_patch(patch_text):\n",
        "    old_lines = []\n",
        "    new_lines = []\n",
        "\n",
        "    for line in patch_text.splitlines():\n",
        "        if line.startswith('@@'):\n",
        "            # diff hunk header, ignore or you can parse line ranges if needed\n",
        "            continue\n",
        "        elif line.startswith('-'):\n",
        "            # removed line: add to old code only\n",
        "            old_lines.append(line[1:])\n",
        "        elif line.startswith('+'):\n",
        "            # added line: add to new code only\n",
        "            new_lines.append(line[1:])\n",
        "        else:\n",
        "            # context line, add to both old and new\n",
        "            old_lines.append(line)\n",
        "            new_lines.append(line)\n",
        "\n",
        "    old_code = \"\\n\".join(old_lines)\n",
        "    new_code = \"\\n\".join(new_lines)\n",
        "\n",
        "    return old_code, new_code\n"
      ],
      "metadata": {
        "id": "rcsrDfTMfi_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation(batch):\n",
        "  conversations = []\n",
        "\n",
        "  for problem, patch, repo in zip(\n",
        "      batch[\"problem_statement\"],\n",
        "      batch[\"patch\"],\n",
        "      batch[\"repo\"],\n",
        "  ):\n",
        "    prev, new = separate_patch(patch)\n",
        "    user_prompt = f\"\"\"\\\n",
        "  We're currently solving the following issue within our repository. Here's the issue text:\n",
        "  ISSUE:\n",
        "  {problem}\n",
        "  Now, you're going to solve this issue on your own.\n",
        "  The issue is in:\n",
        "  {prev}\n",
        "  YOU MUST RETURN A PATCH\n",
        "          \"\"\"\n",
        "    patch_prompt= f\"\"\"\\\n",
        "  DISCUSSION\n",
        "  The solved code for this problem is:\n",
        "  {new}\n",
        "  <command>\n",
        "  {patch}\n",
        "  </command>\n",
        "          \"\"\"\n",
        "    conversations.append([\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": patch_prompt}\n",
        "        ])\n",
        "\n",
        "  return {\"conversations\": conversations}\n"
      ],
      "metadata": {
        "id": "qXfI4MxfnzYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBFaeQHfSxp"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# Standardize the dataset first\n",
        "dataset = standardize_sharegpt(swe_bench_lite)\n",
        "\n",
        "# Apply chat template with explicit thinking mode\n",
        "swe_bench_lite_conversations = tokenizer.apply_chat_template(\n",
        "    dataset.map(generate_conversation, batched = True)[\"conversations\"],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False  # Explicitly disable thinking mode\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9FcosGvfdNr"
      },
      "source": [
        "Let's see the first row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0hbEekfeqf"
      },
      "outputs": [],
      "source": [
        "swe_bench_lite_conversations[1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0L18QMfot4"
      },
      "source": [
        "Now let's see how long both datasets are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDFuUq1foWj"
      },
      "outputs": [],
      "source": [
        "print(len(swe_bench_lite_conversations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-4prS_gVel"
      },
      "source": [
        "Finally combine both datasets:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.Series(swe_bench_lite_conversations)\n",
        "data.name = \"text\"\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "dataset = dataset.shuffle(seed = 3407)"
      ],
      "metadata": {
        "id": "WT9NTd3ToVOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        # warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 15,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "for _ in range(3):\n",
        "  trainer_stats = trainer.train()\n",
        "  model.save_pretrained(\"lora_model\")\n",
        "  tokenizer.save_pretrained(\"lora_model\")\n",
        "  model.push_to_hub_merged(\n",
        "      \"BoJavs/TrainedQwen2.5\",\n",
        "      tokenizer,\n",
        "      save_method=\"lora\",\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "# model.save_pretrained_merged(\n",
        "#     \"merged_model\",\n",
        "#     tokenizer,\n",
        "#     save_method=\"merged_16bit\",\n",
        "# )\n",
        "\n",
        "# model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "model.push_to_hub_gguf(\n",
        "    \"BoJavs/TrainedQwen2.5-GGUF\",\n",
        "    tokenizer,\n",
        "    quantization_method=[\"q4_k_m\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "prompt = \"\"\"You are an autonomous programmer.\n",
        "ISSUE:\n",
        "Fix this Python bug:\n",
        "\n",
        "def add_numbers(a, b):\n",
        "    return a - b\n",
        "\n",
        "You MUST RETURN A PATCH.\n",
        "\n",
        "<command>\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.convert_tokens_to_ids(\"</command>\"),\n",
        "    )\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "oNUkTxif0qDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply asyncio patch for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load your model (adjust this to your model's loading code)\n",
        "from unsloth import FastLanguageModel  # Or your actual import\n",
        "\n",
        "class RequestBody(BaseModel):\n",
        "    prompt: str\n",
        "    max_tokens: int = 200000\n",
        "    temperature: float = 0.7\n",
        "    stop: str | None = None\n",
        "\n",
        "\n",
        "@app.post(\"/v1/completions\")\n",
        "def generate_completion(data: RequestBody):\n",
        "  inputs = tokenizer(data.prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.convert_tokens_to_ids(\"</command>\"),\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(decoded)\n",
        "\n",
        "    return {\"response\": decoded}\n",
        "# Open the public tunnel\n",
        "ngrok.set_auth_token(userdata.get('ngrok'))\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"ðŸ”— Public URL:\", public_url)\n",
        "\n",
        "# Start the FastAPI app\n",
        "uvicorn.run(app, port=8000)\n"
      ],
      "metadata": {
        "id": "XPca5SEU2gYs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}