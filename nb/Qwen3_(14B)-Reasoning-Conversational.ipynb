{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BoJavs-svg/LLM_Lora_FineTunning/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys3tpOFBeUHf"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "from huggingface_hub import login\n"
      ],
      "metadata": {
        "id": "B7aLHYR32yD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "b7dfefd67062442aa3f379986b111dcd",
            "2b8870befd7642f7ac41886afde4b898",
            "18cf1eedaa9540a88a80e830a2846cbb",
            "45978b368d3541999f4814b4a8a72bfe",
            "657065e088094fe5acf5a3cb9ec36d40",
            "9eaf0015718943b3b6780921c06940f4",
            "4b9fa9bf3d9f4500b9284223ddcb0b9b",
            "4ad96021722b4fea830a5e94723b7a9e",
            "ab3bd73d026e4cb09bed71769d71deee",
            "7ceb0066641e47888b25e2fbc73db993",
            "799b0735ca424503a195d2b4ab2d5752",
            "cf2863da0fdb488f8284181b0b91350d",
            "5eb9873fb701454a9c9d32de7e62287b",
            "810a30428dc14fc6b9671cc32959a8bb",
            "48d07ea11bf24067b9dbca4e43d069fc",
            "9f70cad1f33c441da0b088acb5bc5998",
            "8fa88416efac42afbd31ab7d44d35298",
            "3c59592d16e349acb79dc5394e9a2bd4",
            "460ae6c98c23418ca115d96a80bebaa8",
            "c615ac786f91433697405d135582b500",
            "4d2349d9c97e4e14950aae052f067bc5",
            "8ae100562b75445db2819e037f6eb049",
            "7624852adb4d44d5a4bc5e777979f2cd",
            "974cd2d7cdff4d989ca379a24314f459",
            "ad14f9af8e264cd49100e9dcf554d306",
            "17ad80d9b5bd436abab09309531843bc",
            "9b06390a678a41669ebd156e0a495748",
            "75590bfe9cf64f76b8a97013f2b3d63e",
            "6f96b9a6ba67471ab448d493f0a85e46",
            "b1354955e4384ab7a5fdd6b035fab977",
            "f30ea5eda736435a828f34710494b4d9",
            "48bc31431ebd46e0a83deca3f1808ce2",
            "8f38379c61ea40c5ba94e279f168e6f9",
            "2eb1b5b7a3d34678800f6e262904cf58",
            "fa4fa283c8dc41bfb24067aa7af8837f",
            "bb6ccb6954e54028ba4f9260f700e82f",
            "4411083936984b6f886c43b363edf26f",
            "5a91f21196ca4e1f82379e0ec05ce3b3",
            "25f09f54dfc34982be412ec72f4bc022",
            "abb86766a4594184b5b9891e22fab67a",
            "7299a157c6244ce98ab053b5ca3c2d7d",
            "792371d5749243f09ffdb64dda5716a5",
            "7ae7055695ee4c138e6db2ad33788c93",
            "1f4c4e557a6d44ffaf13c9dce9bc83c0",
            "e792a0bbcddd4ace9295d80d4d4916ad",
            "3f1e666d8cbf4b9b9b932b288b74743f",
            "33e58dc88703414f9fd9cf8feeeb8a38",
            "80d7bd20ae4c48a7ac0eb6d19cb11aeb",
            "cc916f627757467fbd18fed41d03fdf4",
            "ef05d54af87c447487db0afb9894c25e",
            "df008fa9439b46778d46c90a014056c9",
            "d8fc9c1020d6419eb269c40fd29b8b5d",
            "0c0dbeb0dea94dfda13dddad1fa78ef0",
            "951d6297fdfc4fa0936e0b1569328077",
            "2d001a4243574090b6488120526aad0e",
            "52f8f8f31ed24e9d8d61617c6bb223a9",
            "ed9f3b687ee44463961a02aa214fe768",
            "46e2ac22c9784122ac3d92bb540cef3d",
            "26bc402ce6c947669629398fe1947a73",
            "14016283d1d24c81af7d48fb12d41652",
            "5c5f8f6241a748f1af02b7b1b2d2911b",
            "8ef9fd55ca804cefa0638142c1bb2515",
            "88699f1d57c44c8d971d964f03f1acd4",
            "062d008c2e5f418b9e7c0afd2ba2da5d",
            "2a35cd3943ba4a52b037990d835aa6da",
            "e7a774fc66324332823797f6a177de18",
            "37552f1379d74dd1aaec63341751ab64",
            "a27ff511c85446999980ca758136ed10",
            "a52ad570a0d04d21a6aff4b6cc812cfa",
            "870426b9b7464fa9b1c3f73e5d22e648",
            "4f69a9df52e1409fa42ceb7036ceabf4",
            "94b601a6d6fc474bb9618c08c4042c24",
            "f7f95a4c961e4ddfa4e94ce5d211fdf9",
            "fbfd170b60b646aab7fe9066b32cf6d4",
            "73258acb60b24279be5b7f95c14bde93",
            "c9e832c3ec9b47c0b88cdbefba86afe6",
            "abc1db43cfba4124b963a5c466aef15b",
            "fbaa8032407a419ab735916579f3ce9b",
            "bc3efe85ecbb4d128479ac736ace0ba7",
            "1b02d1489a904201959579e6906b0772",
            "2bf27fce17f4426b93305f4e3c882270",
            "e61598a73d3340a4860a8f6e8f1aaf22",
            "eb6267cb6f1846d0abbfe26d62e99e89",
            "bc33ae99b7fb48eda1faa993edb9ac9c",
            "fa2b5c2f663a43cf8657996db7992001",
            "4d7539cf73cc40caba9fd443ae4be086",
            "8d8eeaa36c84419e9ff6ccda5b2c8cad",
            "ffae8d2a58af40ac80426e0607c73e68",
            "8a777db6aba24f17b9848b682deb4d08",
            "9281517473d64b93a2b707fe47a89869",
            "cc0cdc1259a846f8b78f5f4036b8134a",
            "3ada3e2d7b2e4c7d9673bde340bd6a14",
            "e3746e6bea684f899be9657de61bbc20",
            "ef24e4a79b014200a48aff72325e8d3f",
            "ccb76d15c02b4e2a83500bfda01f9fb6",
            "26deaf838e6c46e3a626e24be8d62ce1",
            "08369312f1cf426c95cc0186a029b12e",
            "1e06ba00422c4d25b9ca3d5d03b21724",
            "b9a80b2871b443d4b7f15d6ae91790c5",
            "6708201a975e441180680fa4f754485f",
            "b233953189b64b71999f3ed8dd3090ba",
            "9db1e8a6bfea41198413d2a86a902020",
            "b30c14286da54c99a1dd544aa45ed983",
            "5726dcb24e254221a3fddebe18d320aa",
            "bd7593361dec4b7199c1058a5c848b60",
            "b5cfda4b17d245f6a32b991f7e0a1188",
            "ab52466d06fe4ba9aaf72b5787aa0165",
            "00d16607b27042478400913d7d842324",
            "c12c81ad139a447e962b5b443115910c",
            "0bd2f98b753c4d1fbb97f7b93020a04a",
            "a345682a80cd43e8b25ea02ac9ba51e6",
            "7222462a30a34e5c886c2a002524fc87",
            "2b2382d6fd194303ab76566468ede5b4",
            "f619efd16f934b218376e46d801f798d",
            "c139c1f7769c4c74a40914a75c57129c",
            "e971bc6f25ab4891a65518cf35bed937",
            "50ec0217e1d24d46a8dfbd31ac7d6af6",
            "808f40f53fc24ae493bebc3758b008d6",
            "29390bfcebcb4c8688ff0fe97acfec6e",
            "0bad445e4e96450bbd79d057bf189337",
            "4e1e13acc1de4e1b9c40cc33f1c49ad0",
            "30792c4dea934024b172c2f755298f50",
            "f4e5e07ebc5142109d6e5b7c0c08b66a",
            "5b49edad763e4dc2b36124b328e1a4eb",
            "bc5fd89b602e4de98c0f25c3dfc22fdb",
            "c599bdfe18264998ad67e42c3c835cfb",
            "e1a916edc93c46878c02d134b34da64f",
            "4bd30463542e43039facfa0b891c792f",
            "c9986204da5f414cb7092252cf4d66b9",
            "f80cb427a57941e6ac522e2f05a56b7d",
            "9a5ef5684be64192b9540d9dd34c868c",
            "784b680279c5457690b5745ca68e53fd",
            "92a3eacce5dd484a9d8f8502389a529b",
            "59d9c2cb1dca4117b46fc45a51c127e8",
            "674d9f2b816e475fad6f43419f4ecc66",
            "90f18d244c0346b385e4e6d5421c4553",
            "8d825e6d5f9041eba2d02b51b1d12dc0",
            "784289676fca4c53962b322bf1cee105",
            "0b67d102c9944f56bd87f66110abad4c",
            "873fab909dd541b38e55b79fd5acb16d",
            "04e62f809119473b8d4770b3d543aa5e",
            "8d5cec25b93c4dd69a406f707fa36bd7",
            "fffb71d305c44ac9813cbe67930e2bb3",
            "37e92175bf6147b1a47bf612cd5014a4",
            "0bc8db1d0ffc428a94dc9d1e6251a220",
            "6e7a9477910548b5852192572ff75b38",
            "93b5177eca9b4ce4b2115aa0b538fd20",
            "68eebc13323342b3a5215f238c377cd5",
            "a96e0adebe914d4793937f73b2d7965f",
            "f499a850c73d4a8a96ed9a8d2ecb7d0c",
            "5e41b8e96e6a422c8c4afef8cecba59c",
            "23e0b9b079ba45e7b9339f97471c2a07",
            "d24b8fe72acd4696b670b5a37f5d084b",
            "1f2379a54961462daa7302fee510ca2c"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "40ce1f17-cb7d-4824-e320-6c4acac59a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.9: Fast Qwen2 patching. Transformers: 4.52.2.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/196k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7dfefd67062442aa3f379986b111dcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf2863da0fdb488f8284181b0b91350d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7624852adb4d44d5a4bc5e777979f2cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eb1b5b7a3d34678800f6e262904cf58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e792a0bbcddd4ace9295d80d4d4916ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52f8f8f31ed24e9d8d61617c6bb223a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37552f1379d74dd1aaec63341751ab64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbaa8032407a419ab735916579f3ce9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a777db6aba24f17b9848b682deb4d08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6708201a975e441180680fa4f754485f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a345682a80cd43e8b25ea02ac9ba51e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30792c4dea934024b172c2f755298f50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92a3eacce5dd484a9d8f8502389a529b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.9 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/551M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37e92175bf6147b1a47bf612cd5014a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "672\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load the base model\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-14B\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")\n",
        "\n",
        "# 2. Reapply LoRA the same way as originally done\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    base_model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# 3. Load adapter weights directly\n",
        "model.load_adapter(\"BoJavs/TrainedQwen2.5\", adapter_name=\"default\")\n",
        "model.set_adapter(\"default\")\n",
        "tokenizer.add_special_tokens({\n",
        "    \"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]\n",
        "})\n",
        "\n",
        "# Set the chat template manually (Jinja2-style template)\n",
        "tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
        "{{ '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>\\n' }}\n",
        "{% endfor %}\n",
        "{% if add_generation_prompt %}\n",
        "<|im_start|>assistant\n",
        "{% endif %}\"\"\"\n",
        "# 4. Continue with training or inference\n",
        "print(sum(p.requires_grad for p in model.parameters()))  # Should be > 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b230cc-17ce-46e6-e223-6201cafab18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:\n",
        "\n",
        "1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.\n",
        "\n",
        "2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "918eb4425e00421097e09d22cf11c60a",
            "167411b1117d4f0ba239a2e4b14f3870",
            "d5c34796db1d425987697d3d2db952a8",
            "a19a7959ccb1410a92d06df63731de97",
            "ac2ee90de5e947ff87a9b5a12cfb31c6",
            "6742cad0f99c4dd5ba45289c89ca7b87",
            "a83fe8c620754feebe8af28c24c0ff2d",
            "a2492f31f0194a37b3b2d6ba9cc58c94",
            "cf7578a048914d249dc7e8ffd97baaea",
            "83e5156417134e5fb99bff7dd0887edb",
            "17d9e5977f514afdaa92a8e65e16e61c",
            "72d3a9cea2c44bbba04e829f3eb733b8",
            "8313dfba99eb40319367afd240d92fa2",
            "4a92014af0464485a8dbc957c397d37c",
            "b9a601cecdc14fd5b2694abb9b7db618",
            "5bd4b5a9dd904208ba1cdb5d24f3d653",
            "01f9632c155441f4a17eee8999136af7",
            "5156b755a6044fd3be504576b6b8c3f5",
            "000506ac832c4d158294343db8d1d885",
            "287481cc058c4359952728445a5c77dd",
            "fbebee5ca0174186a98a649f0278afb7",
            "238e81a1d7ad4c8fa71c56caa5c04c57",
            "87568b51886a43028cebf26020cc38e7",
            "0ee82eea645c4777975ab2864f275a01",
            "54ebcefc358549ba99be897ce05ba8e2",
            "454fdba658a64f87b01bea740df51784",
            "65a77c03e1174a96ae5e2b980abf2ba6",
            "1534ff343a854cdd93a1552913125b1f",
            "ec6133f1c62146b5a6c4afa49c6230a4",
            "8b191f763d0c413ea3ff5c23d254d9fc",
            "b108308a1abc48698501f2c2ed1d2b56",
            "9fec4ff58803476ab8d0769f1d0b2c93",
            "804c0e0922064e86a3457f8605e88b47",
            "7e00e1c5f94344d89a2efe6d3b27bd74",
            "2a90a052e80f458f86828a7232da3707",
            "cdaf988d46ec4e749cce194047ee65a8",
            "bff8cf14488c42948e3cad3e6366cebe",
            "77cbf5174cd246aab39a37a2174bb6db",
            "e998bea8edf24c0b97d13873e8705940",
            "eafbcd9cd8334f22a769a43dd17b8f6e",
            "197b37ecf11c4632997de25801231b78",
            "f519a70f2c38408abbfb5718b2507c70",
            "9ef7144644f845b996b4e3eb1a19c354",
            "fdb9495fc8cd4cac8fe03e0fa4dfe2e3",
            "ac93c4ec5dc945fc9b68cf1ba4464f29",
            "6d4b558b27ba4e658ecbfd9bc3cbf6f2",
            "77fc60338bd3473e93999095a02a11f3",
            "a7cd1328b7494987bf4aacc87ddf5a0f",
            "9e798a97aa704203aed63208c51c339d",
            "2ac783bf5ebc4fed9ee9ea61ad78a507",
            "ced60342a915467c9d73fe958a7eab3a",
            "14ff408116b44664b0596f4ae78890f6",
            "82b96aecbbf647beab91793c88ebefbb",
            "e18ca3bfcd004692a0865aa1c2374960",
            "6241fa9b823e4c42a219ca633e9b22c5"
          ]
        },
        "outputId": "b427c106-445f-4b9b-b739-8ec7b75fe8f4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/861 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "918eb4425e00421097e09d22cf11c60a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/739k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72d3a9cea2c44bbba04e829f3eb733b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/172k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87568b51886a43028cebf26020cc38e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/176 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e00e1c5f94344d89a2efe6d3b27bd74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/44 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac93c4ec5dc945fc9b68cf1ba4464f29"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "swe_bench_lite = load_dataset('BoJavs/Clean_SweBench', split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZICZtie3lQ"
      },
      "source": [
        "Let's see the structure of both datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zoaygOAe3I2",
        "outputId": "3e478fc3-3598-46cf-9db8-10ca3fb88e4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['repo', 'instance_id', 'base_commit', 'patch', 'test_patch', 'problem_statement', 'hints_text', 'created_at', 'version', 'FAIL_TO_PASS', 'PASS_TO_PASS', 'environment_setup_commit', 'image_name'],\n",
              "    num_rows: 176\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "swe_bench_lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OMhyEXkfM5e"
      },
      "source": [
        "Next we take the non reasoning dataset and convert it to conversational format as well.\n",
        "\n",
        "> Agregar bloque entrecomillado\n",
        "\n",
        "\n",
        "\n",
        "We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_patch(patch_text):\n",
        "    old_lines = []\n",
        "    new_lines = []\n",
        "\n",
        "    for line in patch_text.splitlines():\n",
        "        if line.startswith('@@'):\n",
        "            # diff hunk header, ignore or you can parse line ranges if needed\n",
        "            continue\n",
        "        elif line.startswith('-'):\n",
        "            # removed line: add to old code only\n",
        "            old_lines.append(line[1:])\n",
        "        elif line.startswith('+'):\n",
        "            # added line: add to new code only\n",
        "            new_lines.append(line[1:])\n",
        "        else:\n",
        "            # context line, add to both old and new\n",
        "            old_lines.append(line)\n",
        "            new_lines.append(line)\n",
        "\n",
        "    old_code = \"\\n\".join(old_lines)\n",
        "    new_code = \"\\n\".join(new_lines)\n",
        "\n",
        "    return old_code, new_code\n"
      ],
      "metadata": {
        "id": "rcsrDfTMfi_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation(batch):\n",
        "    conversations = []\n",
        "\n",
        "    for problem, patch, repo in zip(\n",
        "        batch[\"problem_statement\"],\n",
        "        batch[\"patch\"],\n",
        "        batch[\"repo\"],\n",
        "    ):\n",
        "       prev, new = separate_patch(patch)\n",
        "        user_prompt = f\"\"\"\\\n",
        "We're currently solving the following issue within our repository. Here's the issue text:\n",
        "ISSUE:\n",
        "{problem}\n",
        "Now, you're going to solve this issue on your own.\n",
        "The issue is in:\n",
        "{prev}\n",
        "YOU MUST RETURN A PATCH\n",
        "        \"\"\"\n",
        "        patch= f\"\"\"\\\n",
        "DISCUSSION\n",
        "The solved code for this problem is:\n",
        "{new}\n",
        "<command>\n",
        "{patch}\n",
        "</command>\n",
        "        \"\"\"\n",
        "        conversations.append([\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": patch}\n",
        "        ])\n",
        "\n",
        "    return {\"conversations\": conversations}\n"
      ],
      "metadata": {
        "id": "qXfI4MxfnzYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBFaeQHfSxp",
        "outputId": "d160f67a-7b3a-40da-8142-0a576187be13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "824dc9eb5d56462093e5d9720ee8ae91",
            "3a582ad453984e76a423be062391ee77",
            "1be3f47e9c084276a980129c2cfab73d",
            "574b083f75704f4f83b8d13a478c0df5",
            "03f3ec8016b442058c14ca0384d662a0",
            "913cff0aa70241b781b3c6130c543f54",
            "441621ddb2934339b37624e98938318f",
            "8715881aca2d4c6090a368d83f082296",
            "aa8c41e560024de4af20f44c0ee3e291",
            "959f39e74f6c41cfbd65e3ea2d287283",
            "c04f8f0b47ff4fe6a697966fc4f6dd12"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/176 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "824dc9eb5d56462093e5d9720ee8ae91"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# Standardize the dataset first\n",
        "dataset = standardize_sharegpt(swe_bench_lite)\n",
        "\n",
        "# Apply chat template with explicit thinking mode\n",
        "swe_bench_lite_conversations = tokenizer.apply_chat_template(\n",
        "    dataset.map(generate_conversation, batched = True)[\"conversations\"],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False  # Explicitly disable thinking mode\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9FcosGvfdNr"
      },
      "source": [
        "Let's see the first row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0hbEekfeqf",
        "outputId": "3e23e6d4-e21f-40d1-93ce-5b80be28c93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>user\\nYou are an autonomous programmer, and you\\'re working directly in the command line with a special interface.\\nWe\\'re currently solving the following issue within our repository. Here\\'s the issue text:\\nISSUE:\\n\"default.html\" deprecation warning raised for ManagementForm\\'s\\nDescription\\n\\t\\nI have a project where I never render forms with the {{ form }} expression. However, I\\'m still getting the new template deprecation warning because of the formset management form production, during which the template used is insignificant (only hidden inputs are produced).\\nIs it worth special-casing this and avoid producing the warning for the management forms?\\n\\nNow, you\\'re going to solve this issue on your own.\\nYou need to format your output using one field; command.\\nYour output should always_one_ command field EXACTLY as in the following example:\\n<command>\\nls -a\\n</command>\\nGenerate a patch.\\n        <|im_end|>\\n\\n<|im_start|>assistant\\n        <command>\\n        diff --git a/django/forms/formsets.py b/django/forms/formsets.py\\n--- a/django/forms/formsets.py\\n+++ b/django/forms/formsets.py\\n@@ -32,6 +32,8 @@ class ManagementForm(Form):\\n     as well.\\n     \"\"\"\\n \\n+    template_name = \"django/forms/div.html\"  # RemovedInDjango50Warning.\\n+\\n     TOTAL_FORMS = IntegerField(widget=HiddenInput)\\n     INITIAL_FORMS = IntegerField(widget=HiddenInput)\\n     # MIN_NUM_FORM_COUNT and MAX_NUM_FORM_COUNT are output with the rest of the\\n\\n        </command>\\n        <|im_end|>\\n\\n<|im_start|>assistant\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "swe_bench_lite_conversations[1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0L18QMfot4"
      },
      "source": [
        "Now let's see how long both datasets are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDFuUq1foWj",
        "outputId": "2caa2c3c-8d74-49e7-db99-0a080cfe0a1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176\n"
          ]
        }
      ],
      "source": [
        "print(len(swe_bench_lite_conversations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-4prS_gVel"
      },
      "source": [
        "Finally combine both datasets:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.Series(swe_bench_lite_conversations)\n",
        "data.name = \"text\"\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "dataset = dataset.shuffle(seed = 3407)"
      ],
      "metadata": {
        "id": "WT9NTd3ToVOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL",
        "outputId": "b89ed567-c0dc-4294-a9fb-9bc7162de630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "55fc5f3168044f0e91c0c9cbdcccaf69",
            "76a4f98a8895470abe28dc31106eda7f",
            "ef45ae004a6e494eb246f5eebe04afc2",
            "593108d505a84108add746c4acc93adc",
            "fe7f4b0606a2465486bb46d5d00279de",
            "862509a0a8ff49c8ab9c158478befd4c",
            "d201c5e6b9784a9985f73d29575acbcf",
            "e9bacc4a4a9a4a068da8e29d9f6c732b",
            "54787b29f2cd484889b8d77b31695f1f",
            "d91dc20804e24c76b205c6f446efa7b0",
            "73e918b7d4f64e7c9daac881c2185f0f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/176 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55fc5f3168044f0e91c0c9cbdcccaf69"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        # warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 15,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "outputId": "660fdfb2-3bd1-4005-d909-1270e0a50cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 176 | Num Epochs = 1 | Total steps = 22\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 137,625,600/14,000,000,000 (0.98% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22/22 02:01, Epoch 0.95/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.111700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.106600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.084600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.101100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.093700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.092900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.088500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.133000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.097800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.097100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.094300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.108700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.107500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.102700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.105500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.100400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.123800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.124800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.108400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f7d3d7589493>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lora_model\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Local saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lora_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "for _ in range(50):\n",
        "  print(_)\n",
        "  trainer_stats = trainer.train()\n",
        "  model.save_pretrained(\"lora_model\")  # Local saving\n",
        "  tokenizer.save_pretrained(\"lora_model\")\n",
        "  model.push_to_hub_merged(\n",
        "      \"BoJavs/TrainedQwen2.5\",\n",
        "      tokenizer,\n",
        "      save_method=\"lora\",\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc",
        "outputId": "05d38f6d-fa35-42bf-c28a-5376bf0feb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "39efe201d37d450dbbc4a286b3685231",
            "0a4b71b0efb44f3dbf0a4709e6abe1d2",
            "9b6dfdfa3e284c708b80b1767cbc35fb",
            "7085226ecba9417f834e38e880c0d12f",
            "48b768d95a0a47ba9125b645a3bb922a",
            "7ab810f717c846ba9552787fde736b8d",
            "0388ae2a3fed4ae9a80163a473015170",
            "d7cb78f4590d4ea89e0b29db2e33a82f",
            "accdbcba96e149989685ce6afc70ab6c",
            "171201f55d524d429170237f6ccc4899",
            "a2134418526d479882a5b0cdf981d9fc",
            "46caf7ade6894983b536297235e3617c",
            "6d02f8d0117f4caf881b2ee4a9ad262d",
            "97779a25e6ce4862b74710296adb4b5c",
            "159a039217134f7b97af07343902dbaa",
            "70bd6f954ff844519cc4fb3361c83d33",
            "d156cdca41fb4bf183087585506a9751",
            "3154300d3da94da4a912cf5fe19180c4",
            "f26ddc952e1c4a689a0c1685645fedbe",
            "9d8626923018491db8954bef5af0b1d1",
            "e0604f49c9ee4a2a9fc27895911963e6",
            "bba8f5233ff94a15ab236bca6f405205"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 12.1G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 44.61 out of 83.48 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32/48 [00:00<00:00, 53.35it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:20<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n",
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 50.15 out of 83.48 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:10<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at gguf_model into bf16 GGUF format.\n",
            "The output location will be /content/gguf_model/unsloth.BF16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: gguf_model\n",
            "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.36.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.36.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.37.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.37.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.38.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.38.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.39.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.39.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.40.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.40.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.40.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.40.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.41.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.41.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.41.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.41.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.42.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.42.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.42.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
            "INFO:hf-to-gguf:blk.42.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.42.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.42.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.42.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.42.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.43.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.43.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.43.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.43.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.43.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.43.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.44.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.44.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.44.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.44.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.45.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.45.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.45.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.45.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.46.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.46.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.46.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.46.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.47.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.47.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.47.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.47.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 13824\n",
            "INFO:hf-to-gguf:gguf: head count = 40\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151643\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/gguf_model/unsloth.BF16.gguf: n_tensors = 579, total_size = 29.5G\n",
            "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5G/29.5G [04:54<00:00, 100Mbyte/s] \n",
            "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 5557 (b3a89c3d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/gguf_model/unsloth.BF16.gguf' to '/content/gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 579 tensors from /content/gguf_model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gguf_Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 48\n",
            "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:  241 tensors\n",
            "llama_model_loader: - type bf16:  338 tensors\n",
            "[   1/ 579]                        output.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q6_K .. size =  1485.00 MiB ->   609.08 MiB\n",
            "[   2/ 579]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 579]                    token_embd.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q4_K .. size =  1485.00 MiB ->   417.66 MiB\n",
            "[   4/ 579]                    blk.0.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[   5/ 579]                  blk.0.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[   6/ 579]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   7/ 579]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   8/ 579]                    blk.0.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   9/ 579]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  10/ 579]                    blk.0.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  11/ 579]                  blk.0.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  12/ 579]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  13/ 579]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  14/ 579]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 579]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  16/ 579]                    blk.1.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  17/ 579]                  blk.1.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  18/ 579]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  19/ 579]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  20/ 579]                    blk.1.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 579]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  22/ 579]                    blk.1.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  23/ 579]                  blk.1.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  24/ 579]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  25/ 579]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  26/ 579]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  27/ 579]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  28/ 579]                    blk.2.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  29/ 579]                  blk.2.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  30/ 579]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  31/ 579]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  32/ 579]                    blk.2.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 579]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  34/ 579]                    blk.2.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  35/ 579]                  blk.2.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  36/ 579]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  37/ 579]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  38/ 579]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 579]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  40/ 579]                    blk.3.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  41/ 579]                  blk.3.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  42/ 579]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  43/ 579]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  44/ 579]                    blk.3.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  45/ 579]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  46/ 579]                    blk.3.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  47/ 579]                  blk.3.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  48/ 579]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  49/ 579]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  50/ 579]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 579]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  52/ 579]                    blk.4.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  53/ 579]                  blk.4.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  54/ 579]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  55/ 579]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  56/ 579]                    blk.4.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 579]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  58/ 579]                    blk.4.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  59/ 579]                  blk.4.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  60/ 579]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  61/ 579]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  62/ 579]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  63/ 579]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  64/ 579]                    blk.5.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  65/ 579]                  blk.5.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  66/ 579]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  67/ 579]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  68/ 579]                    blk.5.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 579]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  70/ 579]                    blk.5.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  71/ 579]                  blk.5.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  72/ 579]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  73/ 579]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  74/ 579]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 579]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  76/ 579]                    blk.6.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  77/ 579]                  blk.6.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  78/ 579]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  79/ 579]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  80/ 579]                    blk.6.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  81/ 579]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  82/ 579]                    blk.6.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  83/ 579]                  blk.6.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  84/ 579]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  85/ 579]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  86/ 579]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 579]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  88/ 579]                    blk.7.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  89/ 579]                  blk.7.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  90/ 579]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  91/ 579]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  92/ 579]                    blk.7.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 579]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  94/ 579]                    blk.7.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  95/ 579]                  blk.7.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  96/ 579]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  97/ 579]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  98/ 579]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  99/ 579]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 100/ 579]                    blk.8.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 101/ 579]                  blk.8.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 102/ 579]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 103/ 579]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 104/ 579]                    blk.8.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 579]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 106/ 579]                    blk.8.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 107/ 579]                  blk.8.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 108/ 579]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 109/ 579]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 110/ 579]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 579]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 112/ 579]                    blk.9.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 113/ 579]                  blk.9.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 114/ 579]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 115/ 579]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 116/ 579]                    blk.9.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 117/ 579]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 118/ 579]                    blk.9.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 119/ 579]                  blk.9.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 120/ 579]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 121/ 579]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 122/ 579]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 579]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 124/ 579]                   blk.10.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 125/ 579]                 blk.10.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 126/ 579]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 127/ 579]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 128/ 579]                   blk.10.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 579]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 130/ 579]                   blk.10.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 131/ 579]                 blk.10.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 132/ 579]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 133/ 579]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 134/ 579]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 135/ 579]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 136/ 579]                   blk.11.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 137/ 579]                 blk.11.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 138/ 579]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 139/ 579]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 140/ 579]                   blk.11.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 579]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 142/ 579]                   blk.11.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 143/ 579]                 blk.11.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 144/ 579]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 145/ 579]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 146/ 579]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 579]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 148/ 579]                   blk.12.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 149/ 579]                 blk.12.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 150/ 579]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 151/ 579]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 152/ 579]                   blk.12.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 153/ 579]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 154/ 579]                   blk.12.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 155/ 579]                 blk.12.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 156/ 579]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 157/ 579]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 158/ 579]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 579]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 160/ 579]                   blk.13.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 161/ 579]                 blk.13.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 162/ 579]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 163/ 579]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 164/ 579]                   blk.13.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 579]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 166/ 579]                   blk.13.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 167/ 579]                 blk.13.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 168/ 579]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 169/ 579]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 170/ 579]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 171/ 579]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 172/ 579]                   blk.14.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 173/ 579]                 blk.14.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 174/ 579]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 175/ 579]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 176/ 579]                   blk.14.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 579]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 178/ 579]                   blk.14.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 179/ 579]                 blk.14.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 180/ 579]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 181/ 579]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 182/ 579]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 579]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 184/ 579]                   blk.15.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 185/ 579]                 blk.15.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 186/ 579]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 187/ 579]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 188/ 579]                   blk.15.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 189/ 579]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 190/ 579]                   blk.15.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 191/ 579]                 blk.15.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 192/ 579]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 193/ 579]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 194/ 579]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 579]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 196/ 579]                   blk.16.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 197/ 579]                 blk.16.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 198/ 579]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 199/ 579]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 200/ 579]                   blk.16.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 579]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 202/ 579]                   blk.16.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 203/ 579]                 blk.16.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 204/ 579]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 205/ 579]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 206/ 579]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 207/ 579]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 208/ 579]                   blk.17.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 209/ 579]                 blk.17.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 210/ 579]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 211/ 579]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 212/ 579]                   blk.17.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 579]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 214/ 579]                   blk.17.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 215/ 579]                 blk.17.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 216/ 579]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 217/ 579]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 218/ 579]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 579]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 220/ 579]                   blk.18.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 221/ 579]                 blk.18.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 222/ 579]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 223/ 579]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 224/ 579]                   blk.18.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 225/ 579]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 226/ 579]                   blk.18.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 227/ 579]                 blk.18.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 228/ 579]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 229/ 579]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 230/ 579]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 579]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 232/ 579]                   blk.19.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 233/ 579]                 blk.19.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 234/ 579]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 235/ 579]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 236/ 579]                   blk.19.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 579]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 238/ 579]                   blk.19.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 239/ 579]                 blk.19.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 240/ 579]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 241/ 579]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 242/ 579]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 243/ 579]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 244/ 579]                   blk.20.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 245/ 579]                 blk.20.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 246/ 579]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 247/ 579]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 248/ 579]                   blk.20.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 579]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 250/ 579]                   blk.20.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 251/ 579]                 blk.20.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 252/ 579]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 253/ 579]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 254/ 579]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 579]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 256/ 579]                   blk.21.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 257/ 579]                 blk.21.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 258/ 579]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 259/ 579]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 260/ 579]                   blk.21.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 261/ 579]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 262/ 579]                   blk.21.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 263/ 579]                 blk.21.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 264/ 579]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 265/ 579]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 266/ 579]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 579]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 268/ 579]                   blk.22.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 269/ 579]                 blk.22.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 270/ 579]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 271/ 579]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 272/ 579]                   blk.22.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 579]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 274/ 579]                   blk.22.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 275/ 579]                 blk.22.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 276/ 579]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 277/ 579]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 278/ 579]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 279/ 579]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 280/ 579]                   blk.23.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 281/ 579]                 blk.23.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 282/ 579]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 283/ 579]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 284/ 579]                   blk.23.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 579]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 286/ 579]                   blk.23.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 287/ 579]                 blk.23.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 288/ 579]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 289/ 579]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 290/ 579]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 579]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 292/ 579]                   blk.24.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 293/ 579]                 blk.24.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 294/ 579]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 295/ 579]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 296/ 579]                   blk.24.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 297/ 579]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 298/ 579]                   blk.24.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 299/ 579]                 blk.24.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 300/ 579]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 301/ 579]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 302/ 579]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 579]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 304/ 579]                   blk.25.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 305/ 579]                 blk.25.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 306/ 579]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 307/ 579]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 308/ 579]                   blk.25.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 579]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 310/ 579]                   blk.25.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 311/ 579]                 blk.25.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 312/ 579]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 313/ 579]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 314/ 579]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 315/ 579]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 316/ 579]                   blk.26.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 317/ 579]                 blk.26.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 318/ 579]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 319/ 579]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 320/ 579]                   blk.26.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 579]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 322/ 579]                   blk.26.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 323/ 579]                 blk.26.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 324/ 579]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 325/ 579]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 326/ 579]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 579]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 328/ 579]                   blk.27.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 329/ 579]                 blk.27.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 330/ 579]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 331/ 579]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 332/ 579]                   blk.27.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 333/ 579]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 334/ 579]                   blk.27.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 335/ 579]                 blk.27.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 336/ 579]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 337/ 579]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 338/ 579]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 579]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 340/ 579]                   blk.28.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 341/ 579]                 blk.28.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 342/ 579]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 343/ 579]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 344/ 579]                   blk.28.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 579]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 346/ 579]                   blk.28.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 347/ 579]                 blk.28.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 348/ 579]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 349/ 579]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 350/ 579]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 351/ 579]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 352/ 579]                   blk.29.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 353/ 579]                 blk.29.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 354/ 579]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 355/ 579]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 356/ 579]                   blk.29.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 579]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 358/ 579]                   blk.29.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 359/ 579]                 blk.29.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 360/ 579]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 361/ 579]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 362/ 579]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 579]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 364/ 579]                   blk.30.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 365/ 579]                 blk.30.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 366/ 579]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 367/ 579]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 368/ 579]                   blk.30.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 369/ 579]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 370/ 579]                   blk.30.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 371/ 579]                 blk.30.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 372/ 579]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 373/ 579]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 374/ 579]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 375/ 579]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 376/ 579]                   blk.31.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 377/ 579]                 blk.31.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 378/ 579]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 379/ 579]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 380/ 579]                   blk.31.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 381/ 579]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 382/ 579]                   blk.31.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 383/ 579]                 blk.31.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 384/ 579]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 385/ 579]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 386/ 579]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 387/ 579]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 388/ 579]                   blk.32.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 389/ 579]                 blk.32.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 390/ 579]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 391/ 579]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 392/ 579]                   blk.32.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 393/ 579]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 394/ 579]                   blk.32.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 395/ 579]                 blk.32.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 396/ 579]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 397/ 579]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 398/ 579]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 399/ 579]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 400/ 579]                   blk.33.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 401/ 579]                 blk.33.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 402/ 579]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 403/ 579]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 404/ 579]                   blk.33.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 405/ 579]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 406/ 579]                   blk.33.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 407/ 579]                 blk.33.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 408/ 579]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 409/ 579]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 410/ 579]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 411/ 579]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 412/ 579]                   blk.34.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 413/ 579]                 blk.34.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 414/ 579]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 415/ 579]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 416/ 579]                   blk.34.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 417/ 579]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 418/ 579]                   blk.34.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 419/ 579]                 blk.34.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 420/ 579]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 421/ 579]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 422/ 579]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 423/ 579]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 424/ 579]                   blk.35.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 425/ 579]                 blk.35.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 426/ 579]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 427/ 579]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 428/ 579]                   blk.35.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 429/ 579]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 430/ 579]                   blk.35.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 431/ 579]                 blk.35.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 432/ 579]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 433/ 579]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 434/ 579]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 435/ 579]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 436/ 579]                   blk.36.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 437/ 579]                 blk.36.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 438/ 579]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 439/ 579]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 440/ 579]                   blk.36.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 441/ 579]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 442/ 579]                   blk.36.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 443/ 579]                 blk.36.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 444/ 579]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 445/ 579]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 446/ 579]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 447/ 579]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 448/ 579]                   blk.37.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 449/ 579]                 blk.37.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 450/ 579]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 451/ 579]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 452/ 579]                   blk.37.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 453/ 579]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 454/ 579]                   blk.37.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 455/ 579]                 blk.37.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 456/ 579]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 457/ 579]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 458/ 579]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 459/ 579]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 460/ 579]                   blk.38.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 461/ 579]                 blk.38.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 462/ 579]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 463/ 579]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 464/ 579]                   blk.38.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 465/ 579]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 466/ 579]                   blk.38.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 467/ 579]                 blk.38.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 468/ 579]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 469/ 579]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 470/ 579]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 471/ 579]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 472/ 579]                   blk.39.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 473/ 579]                 blk.39.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 474/ 579]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 475/ 579]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 476/ 579]                   blk.39.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 477/ 579]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 478/ 579]                   blk.39.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 479/ 579]                 blk.39.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 480/ 579]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 481/ 579]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 482/ 579]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 483/ 579]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 484/ 579]                   blk.40.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 485/ 579]                 blk.40.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 486/ 579]              blk.40.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 487/ 579]            blk.40.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 488/ 579]                   blk.40.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 489/ 579]                 blk.40.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 490/ 579]                   blk.40.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 491/ 579]                 blk.40.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 492/ 579]               blk.40.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 493/ 579]               blk.40.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 494/ 579]               blk.40.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 495/ 579]                 blk.40.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 496/ 579]                   blk.41.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 497/ 579]                 blk.41.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 498/ 579]              blk.41.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 499/ 579]            blk.41.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 500/ 579]                   blk.41.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 501/ 579]                 blk.41.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 502/ 579]                   blk.41.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 503/ 579]                 blk.41.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 504/ 579]               blk.41.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 505/ 579]               blk.41.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 506/ 579]               blk.41.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 507/ 579]                 blk.41.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 508/ 579]                   blk.42.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 509/ 579]                 blk.42.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 510/ 579]              blk.42.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 511/ 579]            blk.42.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 512/ 579]                   blk.42.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 513/ 579]                 blk.42.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 514/ 579]                   blk.42.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 515/ 579]                 blk.42.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 516/ 579]               blk.42.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 517/ 579]               blk.42.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 518/ 579]               blk.42.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 519/ 579]                 blk.42.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 520/ 579]                   blk.43.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 521/ 579]                 blk.43.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 522/ 579]              blk.43.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 523/ 579]            blk.43.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 524/ 579]                   blk.43.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 525/ 579]                 blk.43.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 526/ 579]                   blk.43.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 527/ 579]                 blk.43.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 528/ 579]               blk.43.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 529/ 579]               blk.43.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 530/ 579]               blk.43.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 531/ 579]                 blk.43.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 532/ 579]                   blk.44.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 533/ 579]                 blk.44.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 534/ 579]              blk.44.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 535/ 579]            blk.44.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 536/ 579]                   blk.44.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 537/ 579]                 blk.44.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 538/ 579]                   blk.44.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 539/ 579]                 blk.44.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 540/ 579]               blk.44.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 541/ 579]               blk.44.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 542/ 579]               blk.44.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 543/ 579]                 blk.44.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 544/ 579]                   blk.45.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 545/ 579]                 blk.45.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 546/ 579]              blk.45.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 547/ 579]            blk.45.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 548/ 579]                   blk.45.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 549/ 579]                 blk.45.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 550/ 579]                   blk.45.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 551/ 579]                 blk.45.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 552/ 579]               blk.45.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 553/ 579]               blk.45.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 554/ 579]               blk.45.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 555/ 579]                 blk.45.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 556/ 579]                   blk.46.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 557/ 579]                 blk.46.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 558/ 579]              blk.46.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 559/ 579]            blk.46.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 560/ 579]                   blk.46.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 561/ 579]                 blk.46.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 562/ 579]                   blk.46.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 563/ 579]                 blk.46.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 564/ 579]               blk.46.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 565/ 579]               blk.46.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 566/ 579]               blk.46.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 567/ 579]                 blk.46.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 568/ 579]                   blk.47.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 569/ 579]                 blk.47.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 570/ 579]              blk.47.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 571/ 579]            blk.47.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 572/ 579]                   blk.47.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 573/ 579]                 blk.47.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 574/ 579]                   blk.47.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 575/ 579]                 blk.47.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 576/ 579]               blk.47.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 577/ 579]               blk.47.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 578/ 579]               blk.47.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 579/ 579]                 blk.47.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "llama_model_quantize_impl: model size  = 28173.21 MB\n",
            "llama_model_quantize_impl: quant size  =  8566.04 MB\n",
            "\n",
            "main: quantize time = 367235.31 ms\n",
            "main:    total time = 367235.31 ms\n",
            "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.Q4_K_M.gguf\n",
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 50.51 out of 83.48 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:13<00:00,  3.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at BoJavs/TrainedQwen2.5-GGUF into bf16 GGUF format.\n",
            "The output location will be /content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: TrainedQwen2.5-GGUF\n",
            "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.36.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.36.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.37.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.37.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.38.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.38.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.39.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.39.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.40.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.40.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.40.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.40.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.40.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.40.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.41.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.41.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.41.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.41.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.41.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.41.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.42.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.42.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.42.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.42.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> BF16, shape = {5120, 152064}\n",
            "INFO:hf-to-gguf:blk.42.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.42.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.42.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.42.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.42.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.43.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.43.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.43.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.43.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.43.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.43.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.43.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.44.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.44.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.44.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.44.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.44.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.44.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.45.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.45.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.45.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.45.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.45.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.45.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.46.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.46.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.46.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.46.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.46.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.46.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_norm.weight,   torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.47.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {13824, 5120}\n",
            "INFO:hf-to-gguf:blk.47.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.47.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {5120, 13824}\n",
            "INFO:hf-to-gguf:blk.47.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_k.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_k.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_output.weight, torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_q.bias,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_q.weight,      torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.47.attn_v.bias,        torch.bfloat16 --> F32, shape = {1024}\n",
            "INFO:hf-to-gguf:blk.47.attn_v.weight,      torch.bfloat16 --> BF16, shape = {5120, 1024}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 13824\n",
            "INFO:hf-to-gguf:gguf: head count = 40\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151643\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf: n_tensors = 579, total_size = 29.5G\n",
            "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5G/29.5G [04:54<00:00, 100Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 5557 (b3a89c3d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf' to '/content/BoJavs/TrainedQwen2.5-GGUF/unsloth.Q4_K_M.gguf' as Q4_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 579 tensors from /content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = TrainedQwen2.5 GGUF\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 48\n",
            "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:  241 tensors\n",
            "llama_model_loader: - type bf16:  338 tensors\n",
            "[   1/ 579]                        output.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q6_K .. size =  1485.00 MiB ->   609.08 MiB\n",
            "[   2/ 579]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 579]                    token_embd.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q4_K .. size =  1485.00 MiB ->   417.66 MiB\n",
            "[   4/ 579]                    blk.0.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[   5/ 579]                  blk.0.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[   6/ 579]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   7/ 579]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   8/ 579]                    blk.0.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   9/ 579]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  10/ 579]                    blk.0.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  11/ 579]                  blk.0.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  12/ 579]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  13/ 579]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  14/ 579]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 579]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  16/ 579]                    blk.1.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  17/ 579]                  blk.1.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  18/ 579]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  19/ 579]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  20/ 579]                    blk.1.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 579]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  22/ 579]                    blk.1.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  23/ 579]                  blk.1.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  24/ 579]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  25/ 579]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  26/ 579]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  27/ 579]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  28/ 579]                    blk.2.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  29/ 579]                  blk.2.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  30/ 579]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  31/ 579]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  32/ 579]                    blk.2.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 579]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  34/ 579]                    blk.2.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  35/ 579]                  blk.2.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  36/ 579]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  37/ 579]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  38/ 579]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 579]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  40/ 579]                    blk.3.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  41/ 579]                  blk.3.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  42/ 579]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  43/ 579]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  44/ 579]                    blk.3.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  45/ 579]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  46/ 579]                    blk.3.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  47/ 579]                  blk.3.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  48/ 579]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  49/ 579]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  50/ 579]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 579]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  52/ 579]                    blk.4.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  53/ 579]                  blk.4.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  54/ 579]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  55/ 579]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  56/ 579]                    blk.4.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 579]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  58/ 579]                    blk.4.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  59/ 579]                  blk.4.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  60/ 579]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  61/ 579]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  62/ 579]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  63/ 579]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  64/ 579]                    blk.5.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  65/ 579]                  blk.5.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  66/ 579]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  67/ 579]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  68/ 579]                    blk.5.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 579]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  70/ 579]                    blk.5.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  71/ 579]                  blk.5.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[  72/ 579]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[  73/ 579]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  74/ 579]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 579]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  76/ 579]                    blk.6.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  77/ 579]                  blk.6.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  78/ 579]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  79/ 579]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  80/ 579]                    blk.6.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  81/ 579]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  82/ 579]                    blk.6.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  83/ 579]                  blk.6.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  84/ 579]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  85/ 579]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  86/ 579]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 579]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  88/ 579]                    blk.7.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  89/ 579]                  blk.7.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  90/ 579]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  91/ 579]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  92/ 579]                    blk.7.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 579]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  94/ 579]                    blk.7.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  95/ 579]                  blk.7.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[  96/ 579]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  97/ 579]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[  98/ 579]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  99/ 579]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 100/ 579]                    blk.8.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 101/ 579]                  blk.8.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 102/ 579]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 103/ 579]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 104/ 579]                    blk.8.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 579]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 106/ 579]                    blk.8.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 107/ 579]                  blk.8.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 108/ 579]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 109/ 579]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 110/ 579]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 579]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 112/ 579]                    blk.9.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 113/ 579]                  blk.9.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 114/ 579]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 115/ 579]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 116/ 579]                    blk.9.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 117/ 579]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 118/ 579]                    blk.9.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 119/ 579]                  blk.9.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 120/ 579]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 121/ 579]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 122/ 579]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 579]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 124/ 579]                   blk.10.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 125/ 579]                 blk.10.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 126/ 579]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 127/ 579]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 128/ 579]                   blk.10.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 579]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 130/ 579]                   blk.10.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 131/ 579]                 blk.10.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 132/ 579]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 133/ 579]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 134/ 579]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 135/ 579]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 136/ 579]                   blk.11.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 137/ 579]                 blk.11.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 138/ 579]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 139/ 579]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 140/ 579]                   blk.11.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 579]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 142/ 579]                   blk.11.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 143/ 579]                 blk.11.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 144/ 579]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 145/ 579]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 146/ 579]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 579]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 148/ 579]                   blk.12.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 149/ 579]                 blk.12.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 150/ 579]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 151/ 579]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 152/ 579]                   blk.12.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 153/ 579]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 154/ 579]                   blk.12.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 155/ 579]                 blk.12.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 156/ 579]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 157/ 579]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 158/ 579]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 579]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 160/ 579]                   blk.13.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 161/ 579]                 blk.13.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 162/ 579]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 163/ 579]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 164/ 579]                   blk.13.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 579]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 166/ 579]                   blk.13.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 167/ 579]                 blk.13.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 168/ 579]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 169/ 579]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 170/ 579]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 171/ 579]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 172/ 579]                   blk.14.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 173/ 579]                 blk.14.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 174/ 579]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 175/ 579]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 176/ 579]                   blk.14.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 579]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 178/ 579]                   blk.14.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 179/ 579]                 blk.14.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 180/ 579]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 181/ 579]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 182/ 579]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 579]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 184/ 579]                   blk.15.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 185/ 579]                 blk.15.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 186/ 579]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 187/ 579]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 188/ 579]                   blk.15.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 189/ 579]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 190/ 579]                   blk.15.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 191/ 579]                 blk.15.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 192/ 579]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 193/ 579]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 194/ 579]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 579]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 196/ 579]                   blk.16.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 197/ 579]                 blk.16.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 198/ 579]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 199/ 579]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 200/ 579]                   blk.16.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 579]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 202/ 579]                   blk.16.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 203/ 579]                 blk.16.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 204/ 579]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 205/ 579]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 206/ 579]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 207/ 579]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 208/ 579]                   blk.17.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 209/ 579]                 blk.17.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 210/ 579]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 211/ 579]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 212/ 579]                   blk.17.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 579]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 214/ 579]                   blk.17.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 215/ 579]                 blk.17.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 216/ 579]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 217/ 579]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 218/ 579]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 579]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 220/ 579]                   blk.18.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 221/ 579]                 blk.18.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 222/ 579]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 223/ 579]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 224/ 579]                   blk.18.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 225/ 579]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 226/ 579]                   blk.18.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 227/ 579]                 blk.18.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 228/ 579]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 229/ 579]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 230/ 579]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 579]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 232/ 579]                   blk.19.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 233/ 579]                 blk.19.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 234/ 579]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 235/ 579]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 236/ 579]                   blk.19.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 579]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 238/ 579]                   blk.19.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 239/ 579]                 blk.19.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 240/ 579]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 241/ 579]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 242/ 579]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 243/ 579]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 244/ 579]                   blk.20.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 245/ 579]                 blk.20.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 246/ 579]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 247/ 579]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 248/ 579]                   blk.20.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 579]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 250/ 579]                   blk.20.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 251/ 579]                 blk.20.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 252/ 579]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 253/ 579]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 254/ 579]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 579]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 256/ 579]                   blk.21.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 257/ 579]                 blk.21.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 258/ 579]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 259/ 579]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 260/ 579]                   blk.21.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 261/ 579]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 262/ 579]                   blk.21.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 263/ 579]                 blk.21.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 264/ 579]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 265/ 579]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 266/ 579]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 579]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 268/ 579]                   blk.22.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 269/ 579]                 blk.22.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 270/ 579]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 271/ 579]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 272/ 579]                   blk.22.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 579]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 274/ 579]                   blk.22.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 275/ 579]                 blk.22.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 276/ 579]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 277/ 579]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 278/ 579]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 279/ 579]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 280/ 579]                   blk.23.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 281/ 579]                 blk.23.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 282/ 579]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 283/ 579]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 284/ 579]                   blk.23.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 579]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 286/ 579]                   blk.23.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 287/ 579]                 blk.23.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 288/ 579]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 289/ 579]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 290/ 579]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 579]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 292/ 579]                   blk.24.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 293/ 579]                 blk.24.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 294/ 579]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 295/ 579]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 296/ 579]                   blk.24.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 297/ 579]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 298/ 579]                   blk.24.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 299/ 579]                 blk.24.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 300/ 579]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 301/ 579]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 302/ 579]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 579]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 304/ 579]                   blk.25.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 305/ 579]                 blk.25.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 306/ 579]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 307/ 579]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 308/ 579]                   blk.25.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 579]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 310/ 579]                   blk.25.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 311/ 579]                 blk.25.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 312/ 579]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 313/ 579]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 314/ 579]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 315/ 579]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 316/ 579]                   blk.26.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 317/ 579]                 blk.26.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 318/ 579]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 319/ 579]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 320/ 579]                   blk.26.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 579]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 322/ 579]                   blk.26.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 323/ 579]                 blk.26.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 324/ 579]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 325/ 579]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 326/ 579]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 579]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 328/ 579]                   blk.27.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 329/ 579]                 blk.27.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 330/ 579]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 331/ 579]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 332/ 579]                   blk.27.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 333/ 579]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 334/ 579]                   blk.27.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 335/ 579]                 blk.27.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 336/ 579]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 337/ 579]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 338/ 579]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 579]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 340/ 579]                   blk.28.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 341/ 579]                 blk.28.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 342/ 579]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 343/ 579]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 344/ 579]                   blk.28.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 579]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 346/ 579]                   blk.28.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 347/ 579]                 blk.28.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 348/ 579]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 349/ 579]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 350/ 579]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 351/ 579]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 352/ 579]                   blk.29.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 353/ 579]                 blk.29.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 354/ 579]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 355/ 579]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 356/ 579]                   blk.29.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 579]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 358/ 579]                   blk.29.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 359/ 579]                 blk.29.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 360/ 579]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 361/ 579]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 362/ 579]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 579]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 364/ 579]                   blk.30.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 365/ 579]                 blk.30.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 366/ 579]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 367/ 579]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 368/ 579]                   blk.30.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 369/ 579]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 370/ 579]                   blk.30.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 371/ 579]                 blk.30.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 372/ 579]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 373/ 579]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 374/ 579]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 375/ 579]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 376/ 579]                   blk.31.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 377/ 579]                 blk.31.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 378/ 579]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 379/ 579]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 380/ 579]                   blk.31.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 381/ 579]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 382/ 579]                   blk.31.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 383/ 579]                 blk.31.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 384/ 579]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 385/ 579]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 386/ 579]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 387/ 579]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 388/ 579]                   blk.32.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 389/ 579]                 blk.32.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 390/ 579]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 391/ 579]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 392/ 579]                   blk.32.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 393/ 579]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 394/ 579]                   blk.32.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 395/ 579]                 blk.32.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 396/ 579]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 397/ 579]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 398/ 579]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 399/ 579]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 400/ 579]                   blk.33.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 401/ 579]                 blk.33.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 402/ 579]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 403/ 579]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 404/ 579]                   blk.33.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 405/ 579]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 406/ 579]                   blk.33.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 407/ 579]                 blk.33.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 408/ 579]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 409/ 579]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 410/ 579]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 411/ 579]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 412/ 579]                   blk.34.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 413/ 579]                 blk.34.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 414/ 579]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 415/ 579]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 416/ 579]                   blk.34.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 417/ 579]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 418/ 579]                   blk.34.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 419/ 579]                 blk.34.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 420/ 579]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 421/ 579]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 422/ 579]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 423/ 579]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 424/ 579]                   blk.35.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 425/ 579]                 blk.35.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 426/ 579]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 427/ 579]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 428/ 579]                   blk.35.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 429/ 579]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 430/ 579]                   blk.35.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 431/ 579]                 blk.35.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 432/ 579]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 433/ 579]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 434/ 579]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 435/ 579]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 436/ 579]                   blk.36.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 437/ 579]                 blk.36.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 438/ 579]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 439/ 579]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 440/ 579]                   blk.36.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 441/ 579]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 442/ 579]                   blk.36.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 443/ 579]                 blk.36.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 444/ 579]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 445/ 579]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 446/ 579]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 447/ 579]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 448/ 579]                   blk.37.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 449/ 579]                 blk.37.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 450/ 579]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 451/ 579]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 452/ 579]                   blk.37.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 453/ 579]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 454/ 579]                   blk.37.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 455/ 579]                 blk.37.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 456/ 579]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 457/ 579]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 458/ 579]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 459/ 579]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 460/ 579]                   blk.38.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 461/ 579]                 blk.38.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 462/ 579]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 463/ 579]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 464/ 579]                   blk.38.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 465/ 579]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 466/ 579]                   blk.38.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 467/ 579]                 blk.38.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 468/ 579]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 469/ 579]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 470/ 579]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 471/ 579]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 472/ 579]                   blk.39.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 473/ 579]                 blk.39.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 474/ 579]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 475/ 579]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 476/ 579]                   blk.39.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 477/ 579]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 478/ 579]                   blk.39.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 479/ 579]                 blk.39.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 480/ 579]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 481/ 579]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 482/ 579]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 483/ 579]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 484/ 579]                   blk.40.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 485/ 579]                 blk.40.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 486/ 579]              blk.40.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 487/ 579]            blk.40.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 488/ 579]                   blk.40.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 489/ 579]                 blk.40.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 490/ 579]                   blk.40.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 491/ 579]                 blk.40.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 492/ 579]               blk.40.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 493/ 579]               blk.40.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 494/ 579]               blk.40.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 495/ 579]                 blk.40.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 496/ 579]                   blk.41.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 497/ 579]                 blk.41.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 498/ 579]              blk.41.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 499/ 579]            blk.41.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 500/ 579]                   blk.41.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 501/ 579]                 blk.41.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 502/ 579]                   blk.41.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 503/ 579]                 blk.41.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 504/ 579]               blk.41.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 505/ 579]               blk.41.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 506/ 579]               blk.41.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 507/ 579]                 blk.41.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 508/ 579]                   blk.42.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 509/ 579]                 blk.42.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 510/ 579]              blk.42.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 511/ 579]            blk.42.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 512/ 579]                   blk.42.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 513/ 579]                 blk.42.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 514/ 579]                   blk.42.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 515/ 579]                 blk.42.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 516/ 579]               blk.42.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 517/ 579]               blk.42.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 518/ 579]               blk.42.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 519/ 579]                 blk.42.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 520/ 579]                   blk.43.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 521/ 579]                 blk.43.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 522/ 579]              blk.43.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 523/ 579]            blk.43.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 524/ 579]                   blk.43.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 525/ 579]                 blk.43.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 526/ 579]                   blk.43.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 527/ 579]                 blk.43.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 528/ 579]               blk.43.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 529/ 579]               blk.43.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 530/ 579]               blk.43.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 531/ 579]                 blk.43.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 532/ 579]                   blk.44.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 533/ 579]                 blk.44.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 534/ 579]              blk.44.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 535/ 579]            blk.44.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 536/ 579]                   blk.44.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 537/ 579]                 blk.44.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 538/ 579]                   blk.44.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 539/ 579]                 blk.44.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 540/ 579]               blk.44.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 541/ 579]               blk.44.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 542/ 579]               blk.44.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 543/ 579]                 blk.44.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 544/ 579]                   blk.45.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 545/ 579]                 blk.45.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 546/ 579]              blk.45.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 547/ 579]            blk.45.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 548/ 579]                   blk.45.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 549/ 579]                 blk.45.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 550/ 579]                   blk.45.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 551/ 579]                 blk.45.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 552/ 579]               blk.45.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 553/ 579]               blk.45.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 554/ 579]               blk.45.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 555/ 579]                 blk.45.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 556/ 579]                   blk.46.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 557/ 579]                 blk.46.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 558/ 579]              blk.46.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 559/ 579]            blk.46.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 560/ 579]                   blk.46.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 561/ 579]                 blk.46.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 562/ 579]                   blk.46.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 563/ 579]                 blk.46.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 564/ 579]               blk.46.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 565/ 579]               blk.46.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 566/ 579]               blk.46.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 567/ 579]                 blk.46.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 568/ 579]                   blk.47.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 569/ 579]                 blk.47.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q4_K .. size =    10.00 MiB ->     2.81 MiB\n",
            "[ 570/ 579]              blk.47.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 571/ 579]            blk.47.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 572/ 579]                   blk.47.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 573/ 579]                 blk.47.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 574/ 579]                   blk.47.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 575/ 579]                 blk.47.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q6_K .. size =    10.00 MiB ->     4.10 MiB\n",
            "[ 576/ 579]               blk.47.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   135.00 MiB ->    55.37 MiB\n",
            "[ 577/ 579]               blk.47.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "[ 578/ 579]               blk.47.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 579/ 579]                 blk.47.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q4_K .. size =   135.00 MiB ->    37.97 MiB\n",
            "llama_model_quantize_impl: model size  = 28173.21 MB\n",
            "llama_model_quantize_impl: quant size  =  8566.04 MB\n",
            "\n",
            "main: quantize time = 358266.65 ms\n",
            "main:    total time = 358266.65 ms\n",
            "Unsloth: Conversion completed! Output location: /content/BoJavs/TrainedQwen2.5-GGUF/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q8_0. This might take 20 minutes...\n",
            "main: build = 5557 (b3a89c3d)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf' to '/content/BoJavs/TrainedQwen2.5-GGUF/unsloth.Q8_0.gguf' as Q8_0 using 24 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 579 tensors from /content/BoJavs/TrainedQwen2.5-GGUF/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = TrainedQwen2.5 GGUF\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 48\n",
            "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:  241 tensors\n",
            "llama_model_loader: - type bf16:  338 tensors\n",
            "[   1/ 579]                        output.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q8_0 .. size =  1485.00 MiB ->   788.91 MiB\n",
            "[   2/ 579]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 579]                    token_embd.weight - [ 5120, 152064,     1,     1], type =   bf16, converting to q8_0 .. size =  1485.00 MiB ->   788.91 MiB\n",
            "[   4/ 579]                    blk.0.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[   5/ 579]                  blk.0.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[   6/ 579]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   7/ 579]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[   8/ 579]                    blk.0.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   9/ 579]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  10/ 579]                    blk.0.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  11/ 579]                  blk.0.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  12/ 579]                blk.0.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  13/ 579]                blk.0.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  14/ 579]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 579]                  blk.0.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  16/ 579]                    blk.1.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  17/ 579]                  blk.1.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  18/ 579]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  19/ 579]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  20/ 579]                    blk.1.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 579]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  22/ 579]                    blk.1.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  23/ 579]                  blk.1.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  24/ 579]                blk.1.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  25/ 579]                blk.1.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  26/ 579]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  27/ 579]                  blk.1.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  28/ 579]                    blk.2.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  29/ 579]                  blk.2.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  30/ 579]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  31/ 579]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  32/ 579]                    blk.2.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 579]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  34/ 579]                    blk.2.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  35/ 579]                  blk.2.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  36/ 579]                blk.2.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  37/ 579]                blk.2.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  38/ 579]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 579]                  blk.2.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  40/ 579]                    blk.3.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  41/ 579]                  blk.3.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  42/ 579]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  43/ 579]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  44/ 579]                    blk.3.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  45/ 579]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  46/ 579]                    blk.3.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  47/ 579]                  blk.3.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  48/ 579]                blk.3.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  49/ 579]                blk.3.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  50/ 579]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 579]                  blk.3.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  52/ 579]                    blk.4.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  53/ 579]                  blk.4.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  54/ 579]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  55/ 579]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  56/ 579]                    blk.4.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 579]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  58/ 579]                    blk.4.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  59/ 579]                  blk.4.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  60/ 579]                blk.4.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  61/ 579]                blk.4.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  62/ 579]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  63/ 579]                  blk.4.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  64/ 579]                    blk.5.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  65/ 579]                  blk.5.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  66/ 579]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  67/ 579]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  68/ 579]                    blk.5.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 579]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  70/ 579]                    blk.5.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  71/ 579]                  blk.5.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  72/ 579]                blk.5.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  73/ 579]                blk.5.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  74/ 579]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 579]                  blk.5.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  76/ 579]                    blk.6.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  77/ 579]                  blk.6.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  78/ 579]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  79/ 579]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  80/ 579]                    blk.6.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  81/ 579]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  82/ 579]                    blk.6.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  83/ 579]                  blk.6.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  84/ 579]                blk.6.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  85/ 579]                blk.6.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  86/ 579]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 579]                  blk.6.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  88/ 579]                    blk.7.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  89/ 579]                  blk.7.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  90/ 579]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  91/ 579]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  92/ 579]                    blk.7.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 579]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[  94/ 579]                    blk.7.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[  95/ 579]                  blk.7.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[  96/ 579]                blk.7.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  97/ 579]                blk.7.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[  98/ 579]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  99/ 579]                  blk.7.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 100/ 579]                    blk.8.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 101/ 579]                  blk.8.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 102/ 579]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 103/ 579]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 104/ 579]                    blk.8.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 579]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 106/ 579]                    blk.8.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 107/ 579]                  blk.8.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 108/ 579]                blk.8.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 109/ 579]                blk.8.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 110/ 579]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 579]                  blk.8.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 112/ 579]                    blk.9.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 113/ 579]                  blk.9.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 114/ 579]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 115/ 579]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 116/ 579]                    blk.9.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 117/ 579]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 118/ 579]                    blk.9.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 119/ 579]                  blk.9.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 120/ 579]                blk.9.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 121/ 579]                blk.9.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 122/ 579]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 579]                  blk.9.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 124/ 579]                   blk.10.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 125/ 579]                 blk.10.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 126/ 579]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 127/ 579]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 128/ 579]                   blk.10.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 579]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 130/ 579]                   blk.10.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 131/ 579]                 blk.10.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 132/ 579]               blk.10.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 133/ 579]               blk.10.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 134/ 579]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 135/ 579]                 blk.10.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 136/ 579]                   blk.11.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 137/ 579]                 blk.11.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 138/ 579]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 139/ 579]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 140/ 579]                   blk.11.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 579]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 142/ 579]                   blk.11.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 143/ 579]                 blk.11.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 144/ 579]               blk.11.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 145/ 579]               blk.11.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 146/ 579]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 579]                 blk.11.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 148/ 579]                   blk.12.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 149/ 579]                 blk.12.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 150/ 579]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 151/ 579]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 152/ 579]                   blk.12.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 153/ 579]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 154/ 579]                   blk.12.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 155/ 579]                 blk.12.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 156/ 579]               blk.12.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 157/ 579]               blk.12.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 158/ 579]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 579]                 blk.12.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 160/ 579]                   blk.13.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 161/ 579]                 blk.13.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 162/ 579]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 163/ 579]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 164/ 579]                   blk.13.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 579]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 166/ 579]                   blk.13.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 167/ 579]                 blk.13.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 168/ 579]               blk.13.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 169/ 579]               blk.13.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 170/ 579]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 171/ 579]                 blk.13.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 172/ 579]                   blk.14.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 173/ 579]                 blk.14.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 174/ 579]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 175/ 579]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 176/ 579]                   blk.14.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 579]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 178/ 579]                   blk.14.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 179/ 579]                 blk.14.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 180/ 579]               blk.14.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 181/ 579]               blk.14.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 182/ 579]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 579]                 blk.14.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 184/ 579]                   blk.15.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 185/ 579]                 blk.15.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 186/ 579]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 187/ 579]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 188/ 579]                   blk.15.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 189/ 579]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 190/ 579]                   blk.15.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 191/ 579]                 blk.15.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 192/ 579]               blk.15.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 193/ 579]               blk.15.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 194/ 579]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 579]                 blk.15.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 196/ 579]                   blk.16.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 197/ 579]                 blk.16.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 198/ 579]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 199/ 579]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 200/ 579]                   blk.16.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 579]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 202/ 579]                   blk.16.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 203/ 579]                 blk.16.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 204/ 579]               blk.16.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 205/ 579]               blk.16.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 206/ 579]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 207/ 579]                 blk.16.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 208/ 579]                   blk.17.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 209/ 579]                 blk.17.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 210/ 579]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 211/ 579]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 212/ 579]                   blk.17.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 579]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 214/ 579]                   blk.17.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 215/ 579]                 blk.17.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 216/ 579]               blk.17.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 217/ 579]               blk.17.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 218/ 579]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 579]                 blk.17.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 220/ 579]                   blk.18.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 221/ 579]                 blk.18.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 222/ 579]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 223/ 579]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 224/ 579]                   blk.18.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 225/ 579]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 226/ 579]                   blk.18.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 227/ 579]                 blk.18.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 228/ 579]               blk.18.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 229/ 579]               blk.18.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 230/ 579]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 579]                 blk.18.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 232/ 579]                   blk.19.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 233/ 579]                 blk.19.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 234/ 579]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 235/ 579]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 236/ 579]                   blk.19.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 579]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 238/ 579]                   blk.19.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 239/ 579]                 blk.19.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 240/ 579]               blk.19.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 241/ 579]               blk.19.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 242/ 579]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 243/ 579]                 blk.19.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 244/ 579]                   blk.20.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 245/ 579]                 blk.20.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 246/ 579]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 247/ 579]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 248/ 579]                   blk.20.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 579]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 250/ 579]                   blk.20.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 251/ 579]                 blk.20.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 252/ 579]               blk.20.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 253/ 579]               blk.20.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 254/ 579]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 579]                 blk.20.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 256/ 579]                   blk.21.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 257/ 579]                 blk.21.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 258/ 579]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 259/ 579]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 260/ 579]                   blk.21.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 261/ 579]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 262/ 579]                   blk.21.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 263/ 579]                 blk.21.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 264/ 579]               blk.21.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 265/ 579]               blk.21.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 266/ 579]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 579]                 blk.21.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 268/ 579]                   blk.22.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 269/ 579]                 blk.22.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 270/ 579]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 271/ 579]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 272/ 579]                   blk.22.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 579]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 274/ 579]                   blk.22.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 275/ 579]                 blk.22.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 276/ 579]               blk.22.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 277/ 579]               blk.22.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 278/ 579]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 279/ 579]                 blk.22.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 280/ 579]                   blk.23.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 281/ 579]                 blk.23.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 282/ 579]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 283/ 579]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 284/ 579]                   blk.23.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 579]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 286/ 579]                   blk.23.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 287/ 579]                 blk.23.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 288/ 579]               blk.23.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 289/ 579]               blk.23.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 290/ 579]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 579]                 blk.23.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 292/ 579]                   blk.24.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 293/ 579]                 blk.24.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 294/ 579]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 295/ 579]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 296/ 579]                   blk.24.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 297/ 579]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 298/ 579]                   blk.24.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 299/ 579]                 blk.24.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 300/ 579]               blk.24.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 301/ 579]               blk.24.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 302/ 579]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 579]                 blk.24.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 304/ 579]                   blk.25.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 305/ 579]                 blk.25.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 306/ 579]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 307/ 579]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 308/ 579]                   blk.25.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 579]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 310/ 579]                   blk.25.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 311/ 579]                 blk.25.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 312/ 579]               blk.25.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 313/ 579]               blk.25.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 314/ 579]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 315/ 579]                 blk.25.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 316/ 579]                   blk.26.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 317/ 579]                 blk.26.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 318/ 579]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 319/ 579]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 320/ 579]                   blk.26.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 579]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 322/ 579]                   blk.26.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 323/ 579]                 blk.26.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 324/ 579]               blk.26.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 325/ 579]               blk.26.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 326/ 579]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 579]                 blk.26.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 328/ 579]                   blk.27.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 329/ 579]                 blk.27.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 330/ 579]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 331/ 579]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 332/ 579]                   blk.27.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 333/ 579]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 334/ 579]                   blk.27.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 335/ 579]                 blk.27.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 336/ 579]               blk.27.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 337/ 579]               blk.27.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 338/ 579]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 579]                 blk.27.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 340/ 579]                   blk.28.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 341/ 579]                 blk.28.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 342/ 579]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 343/ 579]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 344/ 579]                   blk.28.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 579]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 346/ 579]                   blk.28.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 347/ 579]                 blk.28.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 348/ 579]               blk.28.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 349/ 579]               blk.28.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 350/ 579]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 351/ 579]                 blk.28.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 352/ 579]                   blk.29.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 353/ 579]                 blk.29.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 354/ 579]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 355/ 579]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 356/ 579]                   blk.29.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 579]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 358/ 579]                   blk.29.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 359/ 579]                 blk.29.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 360/ 579]               blk.29.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 361/ 579]               blk.29.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 362/ 579]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 579]                 blk.29.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 364/ 579]                   blk.30.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 365/ 579]                 blk.30.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 366/ 579]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 367/ 579]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 368/ 579]                   blk.30.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 369/ 579]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 370/ 579]                   blk.30.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 371/ 579]                 blk.30.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 372/ 579]               blk.30.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 373/ 579]               blk.30.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 374/ 579]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 375/ 579]                 blk.30.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 376/ 579]                   blk.31.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 377/ 579]                 blk.31.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 378/ 579]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 379/ 579]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 380/ 579]                   blk.31.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 381/ 579]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 382/ 579]                   blk.31.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 383/ 579]                 blk.31.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 384/ 579]               blk.31.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 385/ 579]               blk.31.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 386/ 579]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 387/ 579]                 blk.31.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 388/ 579]                   blk.32.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 389/ 579]                 blk.32.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 390/ 579]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 391/ 579]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 392/ 579]                   blk.32.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 393/ 579]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 394/ 579]                   blk.32.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 395/ 579]                 blk.32.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 396/ 579]               blk.32.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 397/ 579]               blk.32.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 398/ 579]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 399/ 579]                 blk.32.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 400/ 579]                   blk.33.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 401/ 579]                 blk.33.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 402/ 579]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 403/ 579]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 404/ 579]                   blk.33.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 405/ 579]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 406/ 579]                   blk.33.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 407/ 579]                 blk.33.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 408/ 579]               blk.33.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 409/ 579]               blk.33.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 410/ 579]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 411/ 579]                 blk.33.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 412/ 579]                   blk.34.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 413/ 579]                 blk.34.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 414/ 579]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 415/ 579]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 416/ 579]                   blk.34.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 417/ 579]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 418/ 579]                   blk.34.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 419/ 579]                 blk.34.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 420/ 579]               blk.34.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 421/ 579]               blk.34.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 422/ 579]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 423/ 579]                 blk.34.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 424/ 579]                   blk.35.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 425/ 579]                 blk.35.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 426/ 579]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 427/ 579]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 428/ 579]                   blk.35.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 429/ 579]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 430/ 579]                   blk.35.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 431/ 579]                 blk.35.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 432/ 579]               blk.35.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 433/ 579]               blk.35.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 434/ 579]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 435/ 579]                 blk.35.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 436/ 579]                   blk.36.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 437/ 579]                 blk.36.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 438/ 579]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 439/ 579]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 440/ 579]                   blk.36.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 441/ 579]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 442/ 579]                   blk.36.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 443/ 579]                 blk.36.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 444/ 579]               blk.36.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 445/ 579]               blk.36.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 446/ 579]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 447/ 579]                 blk.36.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 448/ 579]                   blk.37.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 449/ 579]                 blk.37.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 450/ 579]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 451/ 579]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 452/ 579]                   blk.37.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 453/ 579]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 454/ 579]                   blk.37.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 455/ 579]                 blk.37.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 456/ 579]               blk.37.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 457/ 579]               blk.37.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 458/ 579]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 459/ 579]                 blk.37.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 460/ 579]                   blk.38.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 461/ 579]                 blk.38.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 462/ 579]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 463/ 579]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 464/ 579]                   blk.38.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 465/ 579]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 466/ 579]                   blk.38.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 467/ 579]                 blk.38.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 468/ 579]               blk.38.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 469/ 579]               blk.38.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 470/ 579]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 471/ 579]                 blk.38.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 472/ 579]                   blk.39.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 473/ 579]                 blk.39.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 474/ 579]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 475/ 579]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 476/ 579]                   blk.39.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 477/ 579]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 478/ 579]                   blk.39.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 479/ 579]                 blk.39.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 480/ 579]               blk.39.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 481/ 579]               blk.39.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 482/ 579]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 483/ 579]                 blk.39.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 484/ 579]                   blk.40.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 485/ 579]                 blk.40.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 486/ 579]              blk.40.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 487/ 579]            blk.40.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 488/ 579]                   blk.40.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 489/ 579]                 blk.40.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 490/ 579]                   blk.40.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 491/ 579]                 blk.40.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 492/ 579]               blk.40.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 493/ 579]               blk.40.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 494/ 579]               blk.40.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 495/ 579]                 blk.40.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 496/ 579]                   blk.41.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 497/ 579]                 blk.41.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 498/ 579]              blk.41.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 499/ 579]            blk.41.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 500/ 579]                   blk.41.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 501/ 579]                 blk.41.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 502/ 579]                   blk.41.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 503/ 579]                 blk.41.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 504/ 579]               blk.41.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 505/ 579]               blk.41.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 506/ 579]               blk.41.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 507/ 579]                 blk.41.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 508/ 579]                   blk.42.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 509/ 579]                 blk.42.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 510/ 579]              blk.42.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 511/ 579]            blk.42.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 512/ 579]                   blk.42.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 513/ 579]                 blk.42.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 514/ 579]                   blk.42.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 515/ 579]                 blk.42.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 516/ 579]               blk.42.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 517/ 579]               blk.42.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 518/ 579]               blk.42.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 519/ 579]                 blk.42.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 520/ 579]                   blk.43.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 521/ 579]                 blk.43.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 522/ 579]              blk.43.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 523/ 579]            blk.43.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 524/ 579]                   blk.43.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 525/ 579]                 blk.43.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 526/ 579]                   blk.43.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 527/ 579]                 blk.43.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 528/ 579]               blk.43.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 529/ 579]               blk.43.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 530/ 579]               blk.43.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 531/ 579]                 blk.43.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 532/ 579]                   blk.44.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 533/ 579]                 blk.44.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 534/ 579]              blk.44.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 535/ 579]            blk.44.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 536/ 579]                   blk.44.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 537/ 579]                 blk.44.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 538/ 579]                   blk.44.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 539/ 579]                 blk.44.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 540/ 579]               blk.44.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 541/ 579]               blk.44.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 542/ 579]               blk.44.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 543/ 579]                 blk.44.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 544/ 579]                   blk.45.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 545/ 579]                 blk.45.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 546/ 579]              blk.45.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 547/ 579]            blk.45.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 548/ 579]                   blk.45.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 549/ 579]                 blk.45.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 550/ 579]                   blk.45.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 551/ 579]                 blk.45.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 552/ 579]               blk.45.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 553/ 579]               blk.45.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 554/ 579]               blk.45.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 555/ 579]                 blk.45.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 556/ 579]                   blk.46.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 557/ 579]                 blk.46.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 558/ 579]              blk.46.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 559/ 579]            blk.46.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 560/ 579]                   blk.46.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 561/ 579]                 blk.46.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 562/ 579]                   blk.46.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 563/ 579]                 blk.46.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 564/ 579]               blk.46.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 565/ 579]               blk.46.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 566/ 579]               blk.46.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 567/ 579]                 blk.46.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 568/ 579]                   blk.47.attn_k.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 569/ 579]                 blk.47.attn_k.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 570/ 579]              blk.47.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 571/ 579]            blk.47.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 572/ 579]                   blk.47.attn_q.bias - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 573/ 579]                 blk.47.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =    50.00 MiB ->    26.56 MiB\n",
            "[ 574/ 579]                   blk.47.attn_v.bias - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MB\n",
            "[ 575/ 579]                 blk.47.attn_v.weight - [ 5120,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =    10.00 MiB ->     5.31 MiB\n",
            "[ 576/ 579]               blk.47.ffn_down.weight - [13824,  5120,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 577/ 579]               blk.47.ffn_gate.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "[ 578/ 579]               blk.47.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 579/ 579]                 blk.47.ffn_up.weight - [ 5120, 13824,     1,     1], type =   bf16, converting to q8_0 .. size =   135.00 MiB ->    71.72 MiB\n",
            "llama_model_quantize_impl: model size  = 28173.21 MB\n",
            "llama_model_quantize_impl: quant size  = 14968.52 MB\n",
            "\n",
            "main: quantize time = 109884.11 ms\n",
            "main:    total time = 109884.11 ms\n",
            "Unsloth: Conversion completed! Output location: /content/BoJavs/TrainedQwen2.5-GGUF/unsloth.Q8_0.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/8.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39efe201d37d450dbbc4a286b3685231"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/BoJavs/TrainedQwen2.5-GGUF\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q8_0.gguf:   0%|          | 0.00/15.7G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46caf7ade6894983b536297235e3617c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/BoJavs/TrainedQwen2.5-GGUF\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained_merged(\n",
        "    \"merged_model\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        ")\n",
        "\n",
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "model.push_to_hub_gguf(\n",
        "    \"BoJavs/TrainedQwen2.5-GGUF\",\n",
        "    tokenizer,\n",
        "    quantization_method=[\"q4_k_m\"],\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7dfefd67062442aa3f379986b111dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b8870befd7642f7ac41886afde4b898",
              "IPY_MODEL_18cf1eedaa9540a88a80e830a2846cbb",
              "IPY_MODEL_45978b368d3541999f4814b4a8a72bfe"
            ],
            "layout": "IPY_MODEL_657065e088094fe5acf5a3cb9ec36d40"
          }
        },
        "2b8870befd7642f7ac41886afde4b898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eaf0015718943b3b6780921c06940f4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4b9fa9bf3d9f4500b9284223ddcb0b9b",
            "value": "model.safetensors.index.json:â€‡100%"
          }
        },
        "18cf1eedaa9540a88a80e830a2846cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad96021722b4fea830a5e94723b7a9e",
            "max": 195740,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab3bd73d026e4cb09bed71769d71deee",
            "value": 195740
          }
        },
        "45978b368d3541999f4814b4a8a72bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ceb0066641e47888b25e2fbc73db993",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_799b0735ca424503a195d2b4ab2d5752",
            "value": "â€‡196k/196kâ€‡[00:00&lt;00:00,â€‡4.42MB/s]"
          }
        },
        "657065e088094fe5acf5a3cb9ec36d40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eaf0015718943b3b6780921c06940f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9fa9bf3d9f4500b9284223ddcb0b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ad96021722b4fea830a5e94723b7a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab3bd73d026e4cb09bed71769d71deee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ceb0066641e47888b25e2fbc73db993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "799b0735ca424503a195d2b4ab2d5752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf2863da0fdb488f8284181b0b91350d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5eb9873fb701454a9c9d32de7e62287b",
              "IPY_MODEL_810a30428dc14fc6b9671cc32959a8bb",
              "IPY_MODEL_48d07ea11bf24067b9dbca4e43d069fc"
            ],
            "layout": "IPY_MODEL_9f70cad1f33c441da0b088acb5bc5998"
          }
        },
        "5eb9873fb701454a9c9d32de7e62287b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fa88416efac42afbd31ab7d44d35298",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3c59592d16e349acb79dc5394e9a2bd4",
            "value": "Fetchingâ€‡3â€‡files:â€‡100%"
          }
        },
        "810a30428dc14fc6b9671cc32959a8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_460ae6c98c23418ca115d96a80bebaa8",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c615ac786f91433697405d135582b500",
            "value": 3
          }
        },
        "48d07ea11bf24067b9dbca4e43d069fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2349d9c97e4e14950aae052f067bc5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8ae100562b75445db2819e037f6eb049",
            "value": "â€‡3/3â€‡[01:03&lt;00:00,â€‡31.58s/it]"
          }
        },
        "9f70cad1f33c441da0b088acb5bc5998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa88416efac42afbd31ab7d44d35298": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c59592d16e349acb79dc5394e9a2bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "460ae6c98c23418ca115d96a80bebaa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c615ac786f91433697405d135582b500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d2349d9c97e4e14950aae052f067bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae100562b75445db2819e037f6eb049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7624852adb4d44d5a4bc5e777979f2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_974cd2d7cdff4d989ca379a24314f459",
              "IPY_MODEL_ad14f9af8e264cd49100e9dcf554d306",
              "IPY_MODEL_17ad80d9b5bd436abab09309531843bc"
            ],
            "layout": "IPY_MODEL_9b06390a678a41669ebd156e0a495748"
          }
        },
        "974cd2d7cdff4d989ca379a24314f459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75590bfe9cf64f76b8a97013f2b3d63e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6f96b9a6ba67471ab448d493f0a85e46",
            "value": "model-00001-of-00003.safetensors:â€‡100%"
          }
        },
        "ad14f9af8e264cd49100e9dcf554d306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1354955e4384ab7a5fdd6b035fab977",
            "max": 4999890550,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f30ea5eda736435a828f34710494b4d9",
            "value": 4999890550
          }
        },
        "17ad80d9b5bd436abab09309531843bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48bc31431ebd46e0a83deca3f1808ce2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8f38379c61ea40c5ba94e279f168e6f9",
            "value": "â€‡5.00G/5.00Gâ€‡[00:31&lt;00:00,â€‡154MB/s]"
          }
        },
        "9b06390a678a41669ebd156e0a495748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75590bfe9cf64f76b8a97013f2b3d63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f96b9a6ba67471ab448d493f0a85e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1354955e4384ab7a5fdd6b035fab977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30ea5eda736435a828f34710494b4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48bc31431ebd46e0a83deca3f1808ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f38379c61ea40c5ba94e279f168e6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2eb1b5b7a3d34678800f6e262904cf58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa4fa283c8dc41bfb24067aa7af8837f",
              "IPY_MODEL_bb6ccb6954e54028ba4f9260f700e82f",
              "IPY_MODEL_4411083936984b6f886c43b363edf26f"
            ],
            "layout": "IPY_MODEL_5a91f21196ca4e1f82379e0ec05ce3b3"
          }
        },
        "fa4fa283c8dc41bfb24067aa7af8837f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25f09f54dfc34982be412ec72f4bc022",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_abb86766a4594184b5b9891e22fab67a",
            "value": "model-00003-of-00003.safetensors:â€‡100%"
          }
        },
        "bb6ccb6954e54028ba4f9260f700e82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7299a157c6244ce98ab053b5ca3c2d7d",
            "max": 2123882946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_792371d5749243f09ffdb64dda5716a5",
            "value": 2123882946
          }
        },
        "4411083936984b6f886c43b363edf26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ae7055695ee4c138e6db2ad33788c93",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f4c4e557a6d44ffaf13c9dce9bc83c0",
            "value": "â€‡2.12G/2.12Gâ€‡[00:19&lt;00:00,â€‡105MB/s]"
          }
        },
        "5a91f21196ca4e1f82379e0ec05ce3b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f09f54dfc34982be412ec72f4bc022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb86766a4594184b5b9891e22fab67a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7299a157c6244ce98ab053b5ca3c2d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792371d5749243f09ffdb64dda5716a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ae7055695ee4c138e6db2ad33788c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4c4e557a6d44ffaf13c9dce9bc83c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e792a0bbcddd4ace9295d80d4d4916ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f1e666d8cbf4b9b9b932b288b74743f",
              "IPY_MODEL_33e58dc88703414f9fd9cf8feeeb8a38",
              "IPY_MODEL_80d7bd20ae4c48a7ac0eb6d19cb11aeb"
            ],
            "layout": "IPY_MODEL_cc916f627757467fbd18fed41d03fdf4"
          }
        },
        "3f1e666d8cbf4b9b9b932b288b74743f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef05d54af87c447487db0afb9894c25e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_df008fa9439b46778d46c90a014056c9",
            "value": "model-00002-of-00003.safetensors:â€‡100%"
          }
        },
        "33e58dc88703414f9fd9cf8feeeb8a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8fc9c1020d6419eb269c40fd29b8b5d",
            "max": 4979440094,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c0dbeb0dea94dfda13dddad1fa78ef0",
            "value": 4979440094
          }
        },
        "80d7bd20ae4c48a7ac0eb6d19cb11aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951d6297fdfc4fa0936e0b1569328077",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2d001a4243574090b6488120526aad0e",
            "value": "â€‡4.98G/4.98Gâ€‡[01:02&lt;00:00,â€‡61.9MB/s]"
          }
        },
        "cc916f627757467fbd18fed41d03fdf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef05d54af87c447487db0afb9894c25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df008fa9439b46778d46c90a014056c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8fc9c1020d6419eb269c40fd29b8b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0dbeb0dea94dfda13dddad1fa78ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "951d6297fdfc4fa0936e0b1569328077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d001a4243574090b6488120526aad0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52f8f8f31ed24e9d8d61617c6bb223a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed9f3b687ee44463961a02aa214fe768",
              "IPY_MODEL_46e2ac22c9784122ac3d92bb540cef3d",
              "IPY_MODEL_26bc402ce6c947669629398fe1947a73"
            ],
            "layout": "IPY_MODEL_14016283d1d24c81af7d48fb12d41652"
          }
        },
        "ed9f3b687ee44463961a02aa214fe768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c5f8f6241a748f1af02b7b1b2d2911b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8ef9fd55ca804cefa0638142c1bb2515",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "46e2ac22c9784122ac3d92bb540cef3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88699f1d57c44c8d971d964f03f1acd4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_062d008c2e5f418b9e7c0afd2ba2da5d",
            "value": 3
          }
        },
        "26bc402ce6c947669629398fe1947a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a35cd3943ba4a52b037990d835aa6da",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e7a774fc66324332823797f6a177de18",
            "value": "â€‡3/3â€‡[00:03&lt;00:00,â€‡â€‡1.13s/it]"
          }
        },
        "14016283d1d24c81af7d48fb12d41652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c5f8f6241a748f1af02b7b1b2d2911b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef9fd55ca804cefa0638142c1bb2515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88699f1d57c44c8d971d964f03f1acd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "062d008c2e5f418b9e7c0afd2ba2da5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a35cd3943ba4a52b037990d835aa6da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a774fc66324332823797f6a177de18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37552f1379d74dd1aaec63341751ab64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a27ff511c85446999980ca758136ed10",
              "IPY_MODEL_a52ad570a0d04d21a6aff4b6cc812cfa",
              "IPY_MODEL_870426b9b7464fa9b1c3f73e5d22e648"
            ],
            "layout": "IPY_MODEL_4f69a9df52e1409fa42ceb7036ceabf4"
          }
        },
        "a27ff511c85446999980ca758136ed10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94b601a6d6fc474bb9618c08c4042c24",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f7f95a4c961e4ddfa4e94ce5d211fdf9",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "a52ad570a0d04d21a6aff4b6cc812cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbfd170b60b646aab7fe9066b32cf6d4",
            "max": 172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73258acb60b24279be5b7f95c14bde93",
            "value": 172
          }
        },
        "870426b9b7464fa9b1c3f73e5d22e648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9e832c3ec9b47c0b88cdbefba86afe6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_abc1db43cfba4124b963a5c466aef15b",
            "value": "â€‡172/172â€‡[00:00&lt;00:00,â€‡22.3kB/s]"
          }
        },
        "4f69a9df52e1409fa42ceb7036ceabf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b601a6d6fc474bb9618c08c4042c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f95a4c961e4ddfa4e94ce5d211fdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbfd170b60b646aab7fe9066b32cf6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73258acb60b24279be5b7f95c14bde93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9e832c3ec9b47c0b88cdbefba86afe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc1db43cfba4124b963a5c466aef15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbaa8032407a419ab735916579f3ce9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc3efe85ecbb4d128479ac736ace0ba7",
              "IPY_MODEL_1b02d1489a904201959579e6906b0772",
              "IPY_MODEL_2bf27fce17f4426b93305f4e3c882270"
            ],
            "layout": "IPY_MODEL_e61598a73d3340a4860a8f6e8f1aaf22"
          }
        },
        "bc3efe85ecbb4d128479ac736ace0ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6267cb6f1846d0abbfe26d62e99e89",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bc33ae99b7fb48eda1faa993edb9ac9c",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "1b02d1489a904201959579e6906b0772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa2b5c2f663a43cf8657996db7992001",
            "max": 4716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d7539cf73cc40caba9fd443ae4be086",
            "value": 4716
          }
        },
        "2bf27fce17f4426b93305f4e3c882270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d8eeaa36c84419e9ff6ccda5b2c8cad",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ffae8d2a58af40ac80426e0607c73e68",
            "value": "â€‡4.72k/4.72kâ€‡[00:00&lt;00:00,â€‡568kB/s]"
          }
        },
        "e61598a73d3340a4860a8f6e8f1aaf22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6267cb6f1846d0abbfe26d62e99e89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc33ae99b7fb48eda1faa993edb9ac9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa2b5c2f663a43cf8657996db7992001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d7539cf73cc40caba9fd443ae4be086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d8eeaa36c84419e9ff6ccda5b2c8cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffae8d2a58af40ac80426e0607c73e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a777db6aba24f17b9848b682deb4d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9281517473d64b93a2b707fe47a89869",
              "IPY_MODEL_cc0cdc1259a846f8b78f5f4036b8134a",
              "IPY_MODEL_3ada3e2d7b2e4c7d9673bde340bd6a14"
            ],
            "layout": "IPY_MODEL_e3746e6bea684f899be9657de61bbc20"
          }
        },
        "9281517473d64b93a2b707fe47a89869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef24e4a79b014200a48aff72325e8d3f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ccb76d15c02b4e2a83500bfda01f9fb6",
            "value": "vocab.json:â€‡100%"
          }
        },
        "cc0cdc1259a846f8b78f5f4036b8134a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26deaf838e6c46e3a626e24be8d62ce1",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08369312f1cf426c95cc0186a029b12e",
            "value": 2776833
          }
        },
        "3ada3e2d7b2e4c7d9673bde340bd6a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e06ba00422c4d25b9ca3d5d03b21724",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b9a80b2871b443d4b7f15d6ae91790c5",
            "value": "â€‡2.78M/2.78Mâ€‡[00:00&lt;00:00,â€‡13.3MB/s]"
          }
        },
        "e3746e6bea684f899be9657de61bbc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef24e4a79b014200a48aff72325e8d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb76d15c02b4e2a83500bfda01f9fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26deaf838e6c46e3a626e24be8d62ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08369312f1cf426c95cc0186a029b12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e06ba00422c4d25b9ca3d5d03b21724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9a80b2871b443d4b7f15d6ae91790c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6708201a975e441180680fa4f754485f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b233953189b64b71999f3ed8dd3090ba",
              "IPY_MODEL_9db1e8a6bfea41198413d2a86a902020",
              "IPY_MODEL_b30c14286da54c99a1dd544aa45ed983"
            ],
            "layout": "IPY_MODEL_5726dcb24e254221a3fddebe18d320aa"
          }
        },
        "b233953189b64b71999f3ed8dd3090ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd7593361dec4b7199c1058a5c848b60",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b5cfda4b17d245f6a32b991f7e0a1188",
            "value": "merges.txt:â€‡100%"
          }
        },
        "9db1e8a6bfea41198413d2a86a902020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab52466d06fe4ba9aaf72b5787aa0165",
            "max": 1671853,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00d16607b27042478400913d7d842324",
            "value": 1671853
          }
        },
        "b30c14286da54c99a1dd544aa45ed983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12c81ad139a447e962b5b443115910c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0bd2f98b753c4d1fbb97f7b93020a04a",
            "value": "â€‡1.67M/1.67Mâ€‡[00:00&lt;00:00,â€‡24.1MB/s]"
          }
        },
        "5726dcb24e254221a3fddebe18d320aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd7593361dec4b7199c1058a5c848b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5cfda4b17d245f6a32b991f7e0a1188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab52466d06fe4ba9aaf72b5787aa0165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d16607b27042478400913d7d842324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c12c81ad139a447e962b5b443115910c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd2f98b753c4d1fbb97f7b93020a04a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a345682a80cd43e8b25ea02ac9ba51e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7222462a30a34e5c886c2a002524fc87",
              "IPY_MODEL_2b2382d6fd194303ab76566468ede5b4",
              "IPY_MODEL_f619efd16f934b218376e46d801f798d"
            ],
            "layout": "IPY_MODEL_c139c1f7769c4c74a40914a75c57129c"
          }
        },
        "7222462a30a34e5c886c2a002524fc87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e971bc6f25ab4891a65518cf35bed937",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_50ec0217e1d24d46a8dfbd31ac7d6af6",
            "value": "added_tokens.json:â€‡100%"
          }
        },
        "2b2382d6fd194303ab76566468ede5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_808f40f53fc24ae493bebc3758b008d6",
            "max": 605,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29390bfcebcb4c8688ff0fe97acfec6e",
            "value": 605
          }
        },
        "f619efd16f934b218376e46d801f798d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bad445e4e96450bbd79d057bf189337",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4e1e13acc1de4e1b9c40cc33f1c49ad0",
            "value": "â€‡605/605â€‡[00:00&lt;00:00,â€‡75.0kB/s]"
          }
        },
        "c139c1f7769c4c74a40914a75c57129c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e971bc6f25ab4891a65518cf35bed937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ec0217e1d24d46a8dfbd31ac7d6af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "808f40f53fc24ae493bebc3758b008d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29390bfcebcb4c8688ff0fe97acfec6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0bad445e4e96450bbd79d057bf189337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e1e13acc1de4e1b9c40cc33f1c49ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30792c4dea934024b172c2f755298f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4e5e07ebc5142109d6e5b7c0c08b66a",
              "IPY_MODEL_5b49edad763e4dc2b36124b328e1a4eb",
              "IPY_MODEL_bc5fd89b602e4de98c0f25c3dfc22fdb"
            ],
            "layout": "IPY_MODEL_c599bdfe18264998ad67e42c3c835cfb"
          }
        },
        "f4e5e07ebc5142109d6e5b7c0c08b66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1a916edc93c46878c02d134b34da64f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4bd30463542e43039facfa0b891c792f",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "5b49edad763e4dc2b36124b328e1a4eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9986204da5f414cb7092252cf4d66b9",
            "max": 617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f80cb427a57941e6ac522e2f05a56b7d",
            "value": 617
          }
        },
        "bc5fd89b602e4de98c0f25c3dfc22fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5ef5684be64192b9540d9dd34c868c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_784b680279c5457690b5745ca68e53fd",
            "value": "â€‡617/617â€‡[00:00&lt;00:00,â€‡67.6kB/s]"
          }
        },
        "c599bdfe18264998ad67e42c3c835cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a916edc93c46878c02d134b34da64f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd30463542e43039facfa0b891c792f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9986204da5f414cb7092252cf4d66b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f80cb427a57941e6ac522e2f05a56b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a5ef5684be64192b9540d9dd34c868c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "784b680279c5457690b5745ca68e53fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92a3eacce5dd484a9d8f8502389a529b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59d9c2cb1dca4117b46fc45a51c127e8",
              "IPY_MODEL_674d9f2b816e475fad6f43419f4ecc66",
              "IPY_MODEL_90f18d244c0346b385e4e6d5421c4553"
            ],
            "layout": "IPY_MODEL_8d825e6d5f9041eba2d02b51b1d12dc0"
          }
        },
        "59d9c2cb1dca4117b46fc45a51c127e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_784289676fca4c53962b322bf1cee105",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0b67d102c9944f56bd87f66110abad4c",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "674d9f2b816e475fad6f43419f4ecc66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_873fab909dd541b38e55b79fd5acb16d",
            "max": 11421896,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04e62f809119473b8d4770b3d543aa5e",
            "value": 11421896
          }
        },
        "90f18d244c0346b385e4e6d5421c4553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d5cec25b93c4dd69a406f707fa36bd7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fffb71d305c44ac9813cbe67930e2bb3",
            "value": "â€‡11.4M/11.4Mâ€‡[00:00&lt;00:00,â€‡55.2MB/s]"
          }
        },
        "8d825e6d5f9041eba2d02b51b1d12dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "784289676fca4c53962b322bf1cee105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b67d102c9944f56bd87f66110abad4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "873fab909dd541b38e55b79fd5acb16d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04e62f809119473b8d4770b3d543aa5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d5cec25b93c4dd69a406f707fa36bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fffb71d305c44ac9813cbe67930e2bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37e92175bf6147b1a47bf612cd5014a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bc8db1d0ffc428a94dc9d1e6251a220",
              "IPY_MODEL_6e7a9477910548b5852192572ff75b38",
              "IPY_MODEL_93b5177eca9b4ce4b2115aa0b538fd20"
            ],
            "layout": "IPY_MODEL_68eebc13323342b3a5215f238c377cd5"
          }
        },
        "0bc8db1d0ffc428a94dc9d1e6251a220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96e0adebe914d4793937f73b2d7965f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f499a850c73d4a8a96ed9a8d2ecb7d0c",
            "value": "adapter_model.safetensors:â€‡100%"
          }
        },
        "6e7a9477910548b5852192572ff75b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e41b8e96e6a422c8c4afef8cecba59c",
            "max": 550593184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23e0b9b079ba45e7b9339f97471c2a07",
            "value": 550593184
          }
        },
        "93b5177eca9b4ce4b2115aa0b538fd20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24b8fe72acd4696b670b5a37f5d084b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f2379a54961462daa7302fee510ca2c",
            "value": "â€‡551M/551Mâ€‡[00:06&lt;00:00,â€‡81.3MB/s]"
          }
        },
        "68eebc13323342b3a5215f238c377cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a96e0adebe914d4793937f73b2d7965f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f499a850c73d4a8a96ed9a8d2ecb7d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e41b8e96e6a422c8c4afef8cecba59c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e0b9b079ba45e7b9339f97471c2a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d24b8fe72acd4696b670b5a37f5d084b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f2379a54961462daa7302fee510ca2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "918eb4425e00421097e09d22cf11c60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_167411b1117d4f0ba239a2e4b14f3870",
              "IPY_MODEL_d5c34796db1d425987697d3d2db952a8",
              "IPY_MODEL_a19a7959ccb1410a92d06df63731de97"
            ],
            "layout": "IPY_MODEL_ac2ee90de5e947ff87a9b5a12cfb31c6"
          }
        },
        "167411b1117d4f0ba239a2e4b14f3870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6742cad0f99c4dd5ba45289c89ca7b87",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a83fe8c620754feebe8af28c24c0ff2d",
            "value": "README.md:â€‡100%"
          }
        },
        "d5c34796db1d425987697d3d2db952a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2492f31f0194a37b3b2d6ba9cc58c94",
            "max": 861,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf7578a048914d249dc7e8ffd97baaea",
            "value": 861
          }
        },
        "a19a7959ccb1410a92d06df63731de97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83e5156417134e5fb99bff7dd0887edb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_17d9e5977f514afdaa92a8e65e16e61c",
            "value": "â€‡861/861â€‡[00:00&lt;00:00,â€‡97.2kB/s]"
          }
        },
        "ac2ee90de5e947ff87a9b5a12cfb31c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6742cad0f99c4dd5ba45289c89ca7b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83fe8c620754feebe8af28c24c0ff2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2492f31f0194a37b3b2d6ba9cc58c94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf7578a048914d249dc7e8ffd97baaea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83e5156417134e5fb99bff7dd0887edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d9e5977f514afdaa92a8e65e16e61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72d3a9cea2c44bbba04e829f3eb733b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8313dfba99eb40319367afd240d92fa2",
              "IPY_MODEL_4a92014af0464485a8dbc957c397d37c",
              "IPY_MODEL_b9a601cecdc14fd5b2694abb9b7db618"
            ],
            "layout": "IPY_MODEL_5bd4b5a9dd904208ba1cdb5d24f3d653"
          }
        },
        "8313dfba99eb40319367afd240d92fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01f9632c155441f4a17eee8999136af7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5156b755a6044fd3be504576b6b8c3f5",
            "value": "train-00000-of-00001.parquet:â€‡100%"
          }
        },
        "4a92014af0464485a8dbc957c397d37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000506ac832c4d158294343db8d1d885",
            "max": 739034,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_287481cc058c4359952728445a5c77dd",
            "value": 739034
          }
        },
        "b9a601cecdc14fd5b2694abb9b7db618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbebee5ca0174186a98a649f0278afb7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_238e81a1d7ad4c8fa71c56caa5c04c57",
            "value": "â€‡739k/739kâ€‡[00:00&lt;00:00,â€‡71.6MB/s]"
          }
        },
        "5bd4b5a9dd904208ba1cdb5d24f3d653": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01f9632c155441f4a17eee8999136af7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5156b755a6044fd3be504576b6b8c3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "000506ac832c4d158294343db8d1d885": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287481cc058c4359952728445a5c77dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbebee5ca0174186a98a649f0278afb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238e81a1d7ad4c8fa71c56caa5c04c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87568b51886a43028cebf26020cc38e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ee82eea645c4777975ab2864f275a01",
              "IPY_MODEL_54ebcefc358549ba99be897ce05ba8e2",
              "IPY_MODEL_454fdba658a64f87b01bea740df51784"
            ],
            "layout": "IPY_MODEL_65a77c03e1174a96ae5e2b980abf2ba6"
          }
        },
        "0ee82eea645c4777975ab2864f275a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1534ff343a854cdd93a1552913125b1f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ec6133f1c62146b5a6c4afa49c6230a4",
            "value": "test-00000-of-00001.parquet:â€‡100%"
          }
        },
        "54ebcefc358549ba99be897ce05ba8e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b191f763d0c413ea3ff5c23d254d9fc",
            "max": 172237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b108308a1abc48698501f2c2ed1d2b56",
            "value": 172237
          }
        },
        "454fdba658a64f87b01bea740df51784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fec4ff58803476ab8d0769f1d0b2c93",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_804c0e0922064e86a3457f8605e88b47",
            "value": "â€‡172k/172kâ€‡[00:00&lt;00:00,â€‡18.2MB/s]"
          }
        },
        "65a77c03e1174a96ae5e2b980abf2ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1534ff343a854cdd93a1552913125b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec6133f1c62146b5a6c4afa49c6230a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b191f763d0c413ea3ff5c23d254d9fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b108308a1abc48698501f2c2ed1d2b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fec4ff58803476ab8d0769f1d0b2c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "804c0e0922064e86a3457f8605e88b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e00e1c5f94344d89a2efe6d3b27bd74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a90a052e80f458f86828a7232da3707",
              "IPY_MODEL_cdaf988d46ec4e749cce194047ee65a8",
              "IPY_MODEL_bff8cf14488c42948e3cad3e6366cebe"
            ],
            "layout": "IPY_MODEL_77cbf5174cd246aab39a37a2174bb6db"
          }
        },
        "2a90a052e80f458f86828a7232da3707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e998bea8edf24c0b97d13873e8705940",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eafbcd9cd8334f22a769a43dd17b8f6e",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "cdaf988d46ec4e749cce194047ee65a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_197b37ecf11c4632997de25801231b78",
            "max": 176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f519a70f2c38408abbfb5718b2507c70",
            "value": 176
          }
        },
        "bff8cf14488c42948e3cad3e6366cebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef7144644f845b996b4e3eb1a19c354",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fdb9495fc8cd4cac8fe03e0fa4dfe2e3",
            "value": "â€‡176/176â€‡[00:00&lt;00:00,â€‡3299.72â€‡examples/s]"
          }
        },
        "77cbf5174cd246aab39a37a2174bb6db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e998bea8edf24c0b97d13873e8705940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eafbcd9cd8334f22a769a43dd17b8f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "197b37ecf11c4632997de25801231b78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f519a70f2c38408abbfb5718b2507c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ef7144644f845b996b4e3eb1a19c354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb9495fc8cd4cac8fe03e0fa4dfe2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac93c4ec5dc945fc9b68cf1ba4464f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d4b558b27ba4e658ecbfd9bc3cbf6f2",
              "IPY_MODEL_77fc60338bd3473e93999095a02a11f3",
              "IPY_MODEL_a7cd1328b7494987bf4aacc87ddf5a0f"
            ],
            "layout": "IPY_MODEL_9e798a97aa704203aed63208c51c339d"
          }
        },
        "6d4b558b27ba4e658ecbfd9bc3cbf6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ac783bf5ebc4fed9ee9ea61ad78a507",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ced60342a915467c9d73fe958a7eab3a",
            "value": "Generatingâ€‡testâ€‡split:â€‡100%"
          }
        },
        "77fc60338bd3473e93999095a02a11f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14ff408116b44664b0596f4ae78890f6",
            "max": 44,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82b96aecbbf647beab91793c88ebefbb",
            "value": 44
          }
        },
        "a7cd1328b7494987bf4aacc87ddf5a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e18ca3bfcd004692a0865aa1c2374960",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6241fa9b823e4c42a219ca633e9b22c5",
            "value": "â€‡44/44â€‡[00:00&lt;00:00,â€‡2706.08â€‡examples/s]"
          }
        },
        "9e798a97aa704203aed63208c51c339d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac783bf5ebc4fed9ee9ea61ad78a507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced60342a915467c9d73fe958a7eab3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14ff408116b44664b0596f4ae78890f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82b96aecbbf647beab91793c88ebefbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e18ca3bfcd004692a0865aa1c2374960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6241fa9b823e4c42a219ca633e9b22c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "824dc9eb5d56462093e5d9720ee8ae91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a582ad453984e76a423be062391ee77",
              "IPY_MODEL_1be3f47e9c084276a980129c2cfab73d",
              "IPY_MODEL_574b083f75704f4f83b8d13a478c0df5"
            ],
            "layout": "IPY_MODEL_03f3ec8016b442058c14ca0384d662a0"
          }
        },
        "3a582ad453984e76a423be062391ee77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_913cff0aa70241b781b3c6130c543f54",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_441621ddb2934339b37624e98938318f",
            "value": "Map:â€‡100%"
          }
        },
        "1be3f47e9c084276a980129c2cfab73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8715881aca2d4c6090a368d83f082296",
            "max": 176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa8c41e560024de4af20f44c0ee3e291",
            "value": 176
          }
        },
        "574b083f75704f4f83b8d13a478c0df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_959f39e74f6c41cfbd65e3ea2d287283",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c04f8f0b47ff4fe6a697966fc4f6dd12",
            "value": "â€‡176/176â€‡[00:00&lt;00:00,â€‡5715.99â€‡examples/s]"
          }
        },
        "03f3ec8016b442058c14ca0384d662a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913cff0aa70241b781b3c6130c543f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "441621ddb2934339b37624e98938318f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8715881aca2d4c6090a368d83f082296": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8c41e560024de4af20f44c0ee3e291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "959f39e74f6c41cfbd65e3ea2d287283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c04f8f0b47ff4fe6a697966fc4f6dd12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55fc5f3168044f0e91c0c9cbdcccaf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76a4f98a8895470abe28dc31106eda7f",
              "IPY_MODEL_ef45ae004a6e494eb246f5eebe04afc2",
              "IPY_MODEL_593108d505a84108add746c4acc93adc"
            ],
            "layout": "IPY_MODEL_fe7f4b0606a2465486bb46d5d00279de"
          }
        },
        "76a4f98a8895470abe28dc31106eda7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862509a0a8ff49c8ab9c158478befd4c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d201c5e6b9784a9985f73d29575acbcf",
            "value": "Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=12):â€‡100%"
          }
        },
        "ef45ae004a6e494eb246f5eebe04afc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9bacc4a4a9a4a068da8e29d9f6c732b",
            "max": 176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54787b29f2cd484889b8d77b31695f1f",
            "value": 176
          }
        },
        "593108d505a84108add746c4acc93adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d91dc20804e24c76b205c6f446efa7b0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_73e918b7d4f64e7c9daac881c2185f0f",
            "value": "â€‡176/176â€‡[00:02&lt;00:00,â€‡97.47â€‡examples/s]"
          }
        },
        "fe7f4b0606a2465486bb46d5d00279de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "862509a0a8ff49c8ab9c158478befd4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d201c5e6b9784a9985f73d29575acbcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9bacc4a4a9a4a068da8e29d9f6c732b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54787b29f2cd484889b8d77b31695f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d91dc20804e24c76b205c6f446efa7b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73e918b7d4f64e7c9daac881c2185f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39efe201d37d450dbbc4a286b3685231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a4b71b0efb44f3dbf0a4709e6abe1d2",
              "IPY_MODEL_9b6dfdfa3e284c708b80b1767cbc35fb",
              "IPY_MODEL_7085226ecba9417f834e38e880c0d12f"
            ],
            "layout": "IPY_MODEL_48b768d95a0a47ba9125b645a3bb922a"
          }
        },
        "0a4b71b0efb44f3dbf0a4709e6abe1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab810f717c846ba9552787fde736b8d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0388ae2a3fed4ae9a80163a473015170",
            "value": "unsloth.Q4_K_M.gguf:â€‡100%"
          }
        },
        "9b6dfdfa3e284c708b80b1767cbc35fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7cb78f4590d4ea89e0b29db2e33a82f",
            "max": 8988107488,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_accdbcba96e149989685ce6afc70ab6c",
            "value": 8988107488
          }
        },
        "7085226ecba9417f834e38e880c0d12f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171201f55d524d429170237f6ccc4899",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a2134418526d479882a5b0cdf981d9fc",
            "value": "â€‡8.99G/8.99Gâ€‡[03:04&lt;00:00,â€‡42.3MB/s]"
          }
        },
        "48b768d95a0a47ba9125b645a3bb922a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ab810f717c846ba9552787fde736b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0388ae2a3fed4ae9a80163a473015170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7cb78f4590d4ea89e0b29db2e33a82f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "accdbcba96e149989685ce6afc70ab6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "171201f55d524d429170237f6ccc4899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2134418526d479882a5b0cdf981d9fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46caf7ade6894983b536297235e3617c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d02f8d0117f4caf881b2ee4a9ad262d",
              "IPY_MODEL_97779a25e6ce4862b74710296adb4b5c",
              "IPY_MODEL_159a039217134f7b97af07343902dbaa"
            ],
            "layout": "IPY_MODEL_70bd6f954ff844519cc4fb3361c83d33"
          }
        },
        "6d02f8d0117f4caf881b2ee4a9ad262d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d156cdca41fb4bf183087585506a9751",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3154300d3da94da4a912cf5fe19180c4",
            "value": "unsloth.Q8_0.gguf:â€‡100%"
          }
        },
        "97779a25e6ce4862b74710296adb4b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26ddc952e1c4a689a0c1685645fedbe",
            "max": 15701594848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d8626923018491db8954bef5af0b1d1",
            "value": 15701594848
          }
        },
        "159a039217134f7b97af07343902dbaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0604f49c9ee4a2a9fc27895911963e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bba8f5233ff94a15ab236bca6f405205",
            "value": "â€‡15.7G/15.7Gâ€‡[05:17&lt;00:00,â€‡60.6MB/s]"
          }
        },
        "70bd6f954ff844519cc4fb3361c83d33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d156cdca41fb4bf183087585506a9751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3154300d3da94da4a912cf5fe19180c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f26ddc952e1c4a689a0c1685645fedbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d8626923018491db8954bef5af0b1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0604f49c9ee4a2a9fc27895911963e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba8f5233ff94a15ab236bca6f405205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}